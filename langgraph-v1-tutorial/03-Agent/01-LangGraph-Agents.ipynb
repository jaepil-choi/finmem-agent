{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 에이전트 (Agent)\n\n에이전트는 언어 모델(LLM)과 도구(Tool)를 결합하여 복잡한 작업을 수행하는 시스템입니다. 에이전트는 주어진 작업에 대해 추론하고, 필요한 도구를 선택하며, 목표를 향해 반복적으로 작업을 수행합니다.\n\nLangChain의 `create_agent` 함수는 프로덕션 수준의 에이전트 구현을 제공합니다. 이 함수를 사용하면 모델 선택, 도구 연동, 미들웨어 설정 등을 손쉽게 구성할 수 있습니다.\n\n> 참고 문서: [LangGraph Agents](https://docs.langchain.com/oss/python/langgraph/agents.md)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 환경 설정\n\n에이전트 튜토리얼을 시작하기 전에 필요한 환경을 설정합니다. `dotenv`를 사용하여 API 키를 로드하고, `langchain_teddynote`의 로깅 기능을 활성화하여 LangSmith에서 실행 추적을 확인할 수 있도록 합니다.\n\nLangSmith 추적을 활성화하면 에이전트의 추론 과정, 도구 호출, 응답 생성 등을 시각적으로 디버깅할 수 있어 개발에 큰 도움이 됩니다.\n\n아래 코드는 환경 변수를 로드하고 LangSmith 프로젝트를 설정합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(override=True)\n",
    "# 추적을 위한 프로젝트 이름 설정\n",
    "logging.langsmith(\"LangChain-V1-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 모델 (Model)\n\n에이전트의 추론 엔진인 LLM은 `create_agent` 함수의 첫 번째 인자로 전달합니다. 가장 간단한 방법은 `provider:model` 형식의 문자열을 사용하는 것입니다. 이 방식은 빠른 프로토타이핑에 적합합니다.\n\n아래 코드는 모델 식별자 문자열을 사용하여 기본 에이전트를 생성합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# 모델 식별자 문자열을 사용한 간단한 방법\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 모델 세부 설정\n\n더 세밀한 제어가 필요한 경우, 모델 클래스를 직접 인스턴스화하여 다양한 옵션을 설정할 수 있습니다. `temperature`는 응답의 무작위성을, `max_tokens`는 생성할 최대 토큰 수를, `timeout`은 요청 타임아웃을 제어합니다.\n\n아래 코드는 ChatOpenAI 클래스를 사용하여 세부 옵션이 설정된 에이전트를 생성합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 인스턴스를 직접 초기화하여 더 세밀한 제어\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.1,  # 응답의 무작위성 제어\n",
    "    max_tokens=1000,  # 최대 생성 토큰 수\n",
    "    timeout=30,  # 요청 타임아웃(초)\n",
    ")\n",
    "\n",
    "agent = create_agent(model, tools=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 동적 모델 선택\n\n동적 모델 선택은 런타임에 현재 상태와 컨텍스트를 기반으로 사용할 모델을 결정하는 패턴입니다. 이를 통해 정교한 라우팅 로직과 비용 최적화가 가능합니다. 예를 들어, 간단한 질문에는 경량 모델을, 복잡한 대화에는 고급 모델을 사용할 수 있습니다.\n\n`wrap_model_call` 데코레이터를 사용하면 모델 호출 전에 요청을 검사하고 수정할 수 있는 미들웨어를 생성할 수 있습니다.\n\n![](assets/wrap_model_call.png)\n\n아래 코드는 대화 길이에 따라 모델을 동적으로 선택하는 예시입니다."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ModelRequest 속성\n\n`ModelRequest`는 에이전트의 모델 호출 정보를 담는 데이터 클래스로, 미들웨어에서 요청을 검사하고 수정할 때 사용됩니다. `override()` 메서드를 통해 여러 속성을 동시에 변경할 수 있습니다.\n\n아래 코드는 ModelRequest를 사용하여 동적으로 모델과 시스템 프롬프트를 변경하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "# 기본 모델과 고급 모델 정의\n",
    "basic_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"대화 복잡도에 따라 모델 선택\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    # 긴 대화에는 고급 모델 사용\n",
    "    if message_count > 10:\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    return handler(request)\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model, tools=[], middleware=[dynamic_model_selection]  # 기본 모델\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"머신러닝의 동작 원리에 대해서 설명해줘\")]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**ModelRequest 주요 속성:**\n\n| 속성 | 설명 |\n|:---|:---|\n| `model` | 사용할 `BaseChatModel` 인스턴스 |\n| `system_prompt` | 시스템 프롬프트 (선택적) |\n| `messages` | 대화 메시지 리스트 (시스템 프롬프트 제외) |\n| `tool_choice` | 도구 선택 설정 |\n| `tools` | 사용 가능한 도구 리스트 |\n| `response_format` | 응답 형식 지정 |\n| `state` | 현재 에이전트 상태 (`AgentState`) |\n| `runtime` | 에이전트 런타임 정보 |\n| `model_settings` | 추가 모델 설정 (dict) |\n\n아래 코드는 `override()` 메서드를 사용하여 여러 속성을 동시에 변경하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"대화 복잡도에 따라 모델 선택\"\"\"\n",
    "    message_count = len(request.state[\"messages\"][-1].content)\n",
    "    print(f\"글자수: {message_count}\")\n",
    "\n",
    "    # 긴 대화에는 고급 모델 사용\n",
    "    if message_count > 10:\n",
    "        # 여러 속성 동시 변경\n",
    "        new_request = request.override(\n",
    "            model=advanced_model,\n",
    "            system_prompt=\"emoji 를 사용해서 답변해줘\",\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        return handler(new_request)\n",
    "    else:\n",
    "        new_request = request.override(\n",
    "            system_prompt=\"한 문장으로 간결하게 답변해줘. emoji 는 사용하지 말아줘.\",\n",
    "            tool_choice=\"auto\",\n",
    "            model=basic_model,\n",
    "        )\n",
    "        return handler(new_request)\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model, tools=[], middleware=[dynamic_model_selection]  # 기본 모델\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 글자수 기반 모델 선택 테스트\n\n아래는 글자수 10자 미만일 때의 응답입니다. 간결한 답변을 생성하도록 설정되어 있습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(agent, inputs={\"messages\": [HumanMessage(content=\"머신러닝 동작원리\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "아래는 글자수 10자 이상일 때의 응답입니다. 이모지를 사용하여 친근한 답변을 생성하도록 설정되어 있습니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"머신러닝의 동작 원리에 대해서 설명해 주세요.\")\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 프롬프트\n\n에이전트의 동작을 제어하는 핵심 요소 중 하나는 시스템 프롬프트입니다. 시스템 프롬프트를 통해 에이전트의 역할, 응답 스타일, 제약 조건 등을 정의할 수 있습니다."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 시스템 프롬프트\n\n`system_prompt` 매개변수를 사용하여 에이전트의 기본 동작을 정의할 수 있습니다. 시스템 프롬프트는 모든 대화에서 일관되게 적용되며, 에이전트의 페르소나와 응답 가이드라인을 설정하는 데 사용됩니다.\n\n아래 코드는 간결하고 정확한 응답을 생성하도록 시스템 프롬프트를 설정한 에이전트를 생성합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-mini\",\n",
    "    system_prompt=\"You are a helpful assistant. Be concise and accurate.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "아래는 설정된 시스템 프롬프트를 사용한 에이전트의 응답 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"대한민국의 수도는 어디야?\")]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 동적 시스템 프롬프트 (Dynamic Prompting)\n\n런타임 컨텍스트나 에이전트 상태를 기반으로 시스템 프롬프트를 동적으로 생성해야 하는 경우가 있습니다. `dynamic_prompt` 데코레이터를 사용하면 요청마다 다른 시스템 프롬프트를 적용할 수 있습니다.\n\n이 기능은 사용자 역할, 언어 설정, 응답 형식 등을 런타임에 결정해야 할 때 유용합니다. `context_schema`를 정의하여 에이전트 호출 시 필요한 컨텍스트 정보를 전달할 수 있습니다.\n\n아래 코드는 답변 형식과 길이를 동적으로 설정하는 에이전트를 생성합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "class Context(TypedDict):\n",
    "    prompt_type: str\n",
    "    length: int\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"사용자 역할에 따라 시스템 프롬프트 생성\"\"\"\n",
    "    # 답변 형식 설정\n",
    "    answer_type = (\n",
    "        request.runtime.context.get(\"prompt_type\", \"default\")\n",
    "        if request.runtime.context\n",
    "        else \"default\"\n",
    "    )\n",
    "    # 답변 길이 설정\n",
    "    answer_length = (\n",
    "        request.runtime.context.get(\"length\", 20) if request.runtime.context else 20\n",
    "    )\n",
    "    base_prompt = \"You are a helpful assistant. Answer in Korean.\\n\"\n",
    "\n",
    "    # 답변 형식에 따라 시스템 프롬프트 생성(동적 프롬프팅)\n",
    "    if answer_type == \"default\":\n",
    "        return f\"{base_prompt} [답변 형식] 간결하게 답변해줘. 답변 길이는 {answer_length}자 이하로 해줘.\"\n",
    "    elif answer_type == \"sns\":\n",
    "        return f\"{base_prompt} [답변 형식] SNS 형식으로 답변해줘. 답변 길이는 {answer_length}자 이하로 해줘.\"\n",
    "    elif answer_type == \"article\":\n",
    "        return f\"{base_prompt} [답변 형식] 뉴스 기사 형식으로 답변해줘. 답변 길이는 {answer_length}자 이하로 해줘.\"\n",
    "    else:\n",
    "        return f\"{base_prompt} [답변 형식] 간결하게 답변해줘. 답변 길이는 {answer_length}자 이하로 해줘.\"\n",
    "\n",
    "\n",
    "# 컨텍스트 스키마와 user_role_prompt 미들웨어를 사용하여 에이전트 생성\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    middleware=[user_role_prompt],\n",
    "    context_schema=Context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컨텍스트에 따라 시스템 프롬프트가 동적으로 설정됩니다\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"머신러닝의 동작 원리에 대해서 설명해줘\")]\n",
    "    },\n",
    "    context=Context(prompt_type=\"article\", length=1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"머신러닝의 동작 원리에 대해서 설명해줘\")]\n",
    "    },\n",
    "    context=Context(prompt_type=\"sns\", length=50),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 구조화된 답변 출력 (Response Format)\n\n특정 형식으로 에이전트의 출력을 반환하고 싶을 때가 있습니다. LangChain은 `response_format` 매개변수를 통해 구조화된 출력 전략을 제공합니다. 이를 통해 자연어 응답 대신 JSON 객체, Pydantic 모델 등의 형태로 구조화된 데이터를 얻을 수 있습니다.\n\n**지원 타입:**\n- **Pydantic model class**: Pydantic 모델 클래스\n- **`ToolStrategy`**: 도구 기반 구조화 전략 (대부분의 모델에서 작동)\n- **`ProviderStrategy`**: Provider 기반 구조화 전략 (OpenAI 등 지원 모델에서만 작동)\n\n구조화된 응답은 `response_format` 설정에 따라 에이전트 상태의 `structured_response` 키에 반환됩니다."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Pydantic 모델 기반 처리\n\nPydantic 모델을 사용하면 스키마 검증이 자동으로 이루어지며, 필드 설명을 통해 모델이 각 필드의 의미를 이해할 수 있습니다. `Field`의 `description` 매개변수는 모델이 올바른 값을 추출하는 데 도움이 됩니다.\n\n아래 코드는 연락처 정보를 추출하는 Pydantic 모델을 정의합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"The name of the person\")\n",
    "    email: str = Field(description=\"The email of the person\")\n",
    "    phone: str = Field(description=\"The phone number of the person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(model=\"openai:gpt-4.1-mini\", tools=[], response_format=ContactInfo)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: 테디는 AI 엔지니어 입니다. 그의 이메일은 teddy@example.com 이고, 전화번호는 010-1234-5678 입니다.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "아래에서 정형화된 출력 결과를 확인할 수 있습니다. `structured_response` 키에 Pydantic 모델 인스턴스가 반환됩니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ToolStrategy\n\n`ToolStrategy`는 도구 호출(Tool Calling)을 사용하여 구조화된 출력을 생성합니다. 이 방식은 도구 호출을 지원하는 대부분의 모델에서 작동하므로 호환성이 높습니다.\n\n아래 코드는 ToolStrategy를 사용하여 연락처 정보를 추출하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "# 응답 스키마 정의\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\", tools=[], response_format=ToolStrategy(ContactInfo)\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: 테디는 AI 엔지니어 입니다. 그의 이메일은 teddy@example.com 이고, 전화번호는 010-1234-5678 입니다.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ProviderStrategy\n\n`ProviderStrategy`는 모델 제공자의 네이티브 구조화된 출력 기능을 사용합니다. OpenAI와 같이 네이티브 구조화된 출력을 지원하는 제공자에서만 작동하지만, 더 안정적인 결과를 제공합니다.\n\n아래 코드는 ProviderStrategy를 사용하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1\", response_format=ProviderStrategy(ContactInfo)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: 테디는 AI 엔지니어 입니다. 그의 이메일은 teddy@example.com 이고, 전화번호는 010-1234-5678 입니다.\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 미들웨어\n\n미들웨어를 사용하면 모델 호출 전후에 커스텀 로직을 실행할 수 있습니다. `@before_model` 및 `@after_model` 데코레이터를 사용하여 모델 호출을 감싸는 훅을 정의할 수 있습니다.\n\n**미들웨어 활용 사례:**\n- 모델 호출 전 메시지 전처리 (예: 쿼리 재작성)\n- 모델 호출 후 응답 후처리 (예: 필터링, 로깅)\n- 상태 기반 동적 라우팅\n\n아래 코드는 모델 호출 전후에 로깅을 수행하는 미들웨어 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import (\n",
    "    before_model,\n",
    "    after_model,\n",
    ")\n",
    "from langchain.agents.middleware import (\n",
    "    AgentState,\n",
    "    ModelRequest,\n",
    "    ModelResponse,\n",
    "    dynamic_prompt,\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import AIMessage, AnyMessage\n",
    "from langchain_teddynote.messages import invoke_graph\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "# 노드 스타일: 모델 호출 전 로깅\n",
    "@before_model\n",
    "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(\n",
    "        f\"\\033[95m\\n\\n모델 호출 전 메시지 {len(state['messages'])}개가 있습니다\\033[0m\"\n",
    "    )\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    llm = init_chat_model(\"openai:gpt-4.1-mini\")\n",
    "\n",
    "    query_rewrite = (\n",
    "        PromptTemplate.from_template(\n",
    "            \"Rewrite the following query to be more understandable. Do not change the original meaning. Make it one sentence: {query}\"\n",
    "        )\n",
    "        | llm\n",
    "    )\n",
    "    rewritten_query = query_rewrite.invoke({\"query\": last_message})\n",
    "\n",
    "    return {\"messages\": [rewritten_query.content]}\n",
    "\n",
    "\n",
    "@after_model\n",
    "def log_after_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "\n",
    "    print(\n",
    "        f\"\\033[95m\\n\\n모델 호출 후 메시지 {len(state['messages'])}개가 있습니다\\033[0m\"\n",
    "    )\n",
    "    for i, message in enumerate(state[\"messages\"]):\n",
    "        print(f\"[{i}] {message.content}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-mini\",\n",
    "    middleware=[\n",
    "        log_before_model,\n",
    "        log_after_model,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"대한민국 수도\")]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 클래스 기반 미들웨어\n\n데코레이터 대신 클래스 기반 미들웨어를 사용할 수 있습니다. `AgentMiddleware` 클래스를 상속하고 `before_model` 및 `after_model` 메서드를 오버라이드하여 커스텀 로직을 구현합니다.\n\n클래스 기반 미들웨어는 커스텀 상태 스키마를 정의하거나 복잡한 미들웨어 로직을 구조화할 때 유용합니다.\n\n아래 코드는 클래스 기반 미들웨어를 사용하여 커스텀 상태를 관리하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.agents import AgentState\n",
    "from langchain.agents.middleware import AgentMiddleware\n",
    "\n",
    "\n",
    "# 커스텀 상태 스키마 정의\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "\n",
    "class CustomMiddleware(AgentMiddleware):\n",
    "    state_schema = CustomState\n",
    "    tools = []\n",
    "\n",
    "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # 모델 호출 전 커스텀 로직\n",
    "        pass\n",
    "\n",
    "\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], middleware=[CustomMiddleware()])\n",
    "\n",
    "# 에이전트는 이제 메시지 외에 추가 상태를 추적할 수 있습니다\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "        \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 모델 오류 시 재시도 로직\n\n`wrap_model_call` 데코레이터를 사용하면 모델 호출 실패 시 자동으로 재시도하는 로직을 구현할 수 있습니다. 이는 네트워크 오류나 일시적인 API 장애에 대응하는 데 유용합니다.\n\n아래 코드는 최대 3회까지 재시도하는 미들웨어 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def retry_model(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            print(f\"오류 발생으로 {attempt + 1}/3 번째 재시도합니다: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-minis\",  # 일부러 모델 호출 실패하도록 설정(모델명 오류)\n",
    "    middleware=[retry_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"대한민국의 수도는?\")]},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}