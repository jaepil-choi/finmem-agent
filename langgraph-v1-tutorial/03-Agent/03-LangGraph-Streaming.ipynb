{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 메시지 스트리밍\n\n스트리밍을 사용하면 LLM의 출력이 생성되는 대로 점진적으로 표시할 수 있어 더 나은 사용자 경험을 제공합니다. 사용자는 전체 응답이 완료될 때까지 기다리지 않고 실시간으로 결과를 확인할 수 있습니다.\n\nLangChain 에이전트는 기본적으로 스트리밍 모드로 실행되며, 실시간으로 응답을 제공합니다.\n\n**스트리밍 모드 종류:**\n\n| 모드 | 설명 |\n|:---|:---|\n| **updates** | 에이전트의 진행 상황 (기본값) - 각 노드 실행 완료 시 업데이트 |\n| **messages** | LLM의 토큰 스트리밍 - 실시간 텍스트 출력 |\n| **custom** | 커스텀 업데이트 - 도구에서 전송하는 사용자 정의 데이터 |\n\n> 참고 문서: [LangGraph Streaming](https://docs.langchain.com/oss/python/langgraph/streaming.md)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 환경 설정\n\n스트리밍 튜토리얼을 시작하기 전에 필요한 환경을 설정합니다. `dotenv`를 사용하여 API 키를 로드하고, `langchain_teddynote`의 로깅 기능을 활성화하여 LangSmith에서 스트리밍 과정을 추적할 수 있도록 합니다.\n\n아래 코드는 환경 변수를 로드하고 LangSmith 프로젝트를 설정합니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv(override=True)\n",
    "# 추적을 위한 프로젝트 이름 설정\n",
    "logging.langsmith(\"LangChain-V1-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 에이전트 진행 상황 스트리밍\n\n`stream_mode=\"updates\"`는 에이전트의 진행 상황을 추적하는 기본 스트리밍 모드입니다. 각 노드가 실행을 완료할 때마다 업데이트를 생성하며, 노드 이름과 해당 노드의 출력 메시지가 포함됩니다.\n\n이 모드는 에이전트가 어떤 단계를 거치고 있는지 모니터링하거나, 로깅 시스템에 진행 상황을 기록할 때 유용합니다.\n\n아래 코드는 updates 모드로 에이전트를 스트리밍하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'messages': [AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 51, 'total_tokens': 66, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CYEu9Mzrkh34LfQAv66edGfRlapMk', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--dddc9071-2a8e-4ea3-8d13-caec3809dd08-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Seoul'}, 'id': 'call_T4rycmpCSZ3D7PFERZjpj0Fk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 51, 'output_tokens': 15, 'total_tokens': 66, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "---\n",
      "{'tools': {'messages': [ToolMessage(content='The weather in Seoul is sunny!', name='get_weather', id='696e35d5-a206-4573-8d71-75ba5daee5e5', tool_call_id='call_T4rycmpCSZ3D7PFERZjpj0Fk')]}}\n",
      "---\n",
      "{'model': {'messages': [AIMessage(content='The weather in Seoul is sunny! Would you like to know more details about the temperature or other weather conditions?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 81, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CYEuBpL6NZcgEQnC2of6BrDaWnDbI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5f985ba4-3f2f-4679-997f-13db26a9bd14-0', usage_metadata={'input_tokens': 81, 'output_tokens': 23, 'total_tokens': 104, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather for a specific city.\"\"\"\n",
    "    return f\"The weather in {city} is sunny!\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "agent = create_agent(model=model, tools=[get_weather])\n",
    "\n",
    "# 기본 스트리밍 (updates 모드)\n",
    "for chunk in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Seoul?\"}]}):\n",
    "    print(chunk)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## LLM 토큰 스트리밍\n\n`stream_mode=\"messages\"`를 사용하면 LLM이 생성하는 토큰을 실시간으로 스트리밍할 수 있습니다. 이는 사용자에게 즉각적인 피드백을 제공하는 데 유용하며, 채팅 인터페이스에서 타자기 효과를 구현할 때 자주 사용됩니다.\n\n각 청크는 `AIMessageChunk` 객체로 전달되며, `content` 속성에 토큰 텍스트가 포함됩니다.\n\n아래 코드는 messages 모드로 LLM 토큰을 스트리밍하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='In', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' small', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' bustling', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' city', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' there', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' lived', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' robot', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' named', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='iko', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' Unlike', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' other', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' robots', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' designed', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' specific', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' tasks', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='iko', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' was', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' built', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' curious', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' heart', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' mind', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' that', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' loved', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' explore', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' Every', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' day', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='iko', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' wandered', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' through', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' city', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='’s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' parks', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' markets', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' libraries', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' learning', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' about', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' world', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' people', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' living', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' it', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='One', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' day', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' while', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' exploring', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' quiet', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' alley', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='iko', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' found', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' tiny', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' bird', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' broken', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' wing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' Using', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' its', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' gentle', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' hands', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' precise', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' tools', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='iko', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' carefully', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' fixed', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' bird', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='’s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' wing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' bird', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' chir', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='ped', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' happily', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' flutter', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='ed', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' away', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' but', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' it', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' returned', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' every', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' day', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' keep', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='iko', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' company', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='As', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' weeks', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' passed', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='iko', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' became', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' city', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='’s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' gentle', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' helper', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' fixing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' broken', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' things', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' bright', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='ening', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' people', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='’s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' days', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' Though', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' made', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' metal', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' wires', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' T', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='iko', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' showed', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' everyone', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' that', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' kindness', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' curiosity', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' could', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' make', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' robot', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' truly', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content=' special', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1', chunk_position='last'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1', usage_metadata={'input_tokens': 54, 'output_tokens': 173, 'total_tokens': 227, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None})(AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--5046d08b-4aeb-4294-adfa-4d78a3234de1', chunk_position='last'), {'langgraph_step': 1, 'langgraph_node': 'model', 'langgraph_triggers': ('branch:to:model',), 'langgraph_path': ('__pregel_pull', 'model'), 'langgraph_checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'checkpoint_ns': 'model:5970c3b0-7cd1-d55a-a4b4-e3d0994e135a', 'ls_provider': 'openai', 'ls_model_name': 'gpt-4.1-mini', 'ls_model_type': 'chat', 'ls_temperature': None, 'LANGSMITH_PROJECT': 'LangGraph-V1-Tutorial', 'LANGSMITH_TRACING': 'true', 'revision_id': '1449a27-dirty'})"
     ]
    }
   ],
   "source": [
    "# messages 스트리밍 모드\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a short story about a robot.\"}]},\n",
    "    stream_mode=\"messages\"  # LLM 토큰 스트리밍\n",
    "):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 실용적인 예제: 타자기 효과\n\nLLM 토큰을 스트리밍하여 타자기처럼 텍스트를 출력하는 실용적인 예제입니다. `time.sleep()`을 사용하여 각 토큰 사이에 약간의 지연을 추가하면 더 자연스러운 타이핑 효과를 연출할 수 있습니다.\n\n아래 코드는 타자기 효과를 구현하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "print(\"AI: \", end=\"\", flush=True)\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write a haiku about technology.\"}]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    # AIMessageChunk에서 텍스트 추출\n",
    "    if hasattr(chunk, 'content'):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "        time.sleep(0.02)  # 타자기 효과를 위한 약간의 지연\n",
    "\n",
    "print()  # 줄바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 커스텀 업데이트\n\n`runtime.stream_writer` 또는 `runtime.get_stream_writer()`를 사용하면 에이전트 실행 중 커스텀 업데이트를 스트리밍할 수 있습니다. 이는 장시간 실행되는 도구에서 진행 상황, 중간 결과 또는 디버그 정보를 전송하는 데 유용합니다.\n\n커스텀 업데이트는 `stream_mode=\"custom\"`으로 수신할 수 있으며, 도구에서 전송한 데이터 형식 그대로 전달됩니다.\n\n아래 코드는 커스텀 업데이트를 스트리밍하는 도구 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ToolRuntime' object has no attribute 'get_stream_writer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m agent = create_agent(model=model, tools=[process_data])\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 커스텀 스트리밍 모드로 진행 상황 추적\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mProcess 50 items of data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcustom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 커스텀 업데이트 수신\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprogress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpercentage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprogress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:258\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:520\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    518\u001b[39m                 interrupts.append(exc)\n\u001b[32m    519\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/_executor.py:80\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain/tools/tool_node.py:702\u001b[39m, in \u001b[36m_ToolNode._func\u001b[39m\u001b[34m(self, input, config, runtime)\u001b[39m\n\u001b[32m    700\u001b[39m input_types = [input_type] * \u001b[38;5;28mlen\u001b[39m(tool_calls)\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m702\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_runtimes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._combine_tool_outputs(outputs, input_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py:546\u001b[39m, in \u001b[36mContextThreadPoolExecutor.map.<locals>._wrapped_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_fn\u001b[39m(*args: Any) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontexts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain/tools/tool_node.py:911\u001b[39m, in \u001b[36m_ToolNode._run_one\u001b[39m\u001b[34m(self, call, input_type, tool_runtime)\u001b[39m\n\u001b[32m    907\u001b[39m config = tool_runtime.config\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wrap_tool_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    910\u001b[39m     \u001b[38;5;66;03m# No wrapper - execute directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_tool_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[38;5;66;03m# Define execute callable that can be called multiple times\u001b[39;00m\n\u001b[32m    914\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(req: ToolCallRequest) -> ToolMessage | Command:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain/tools/tool_node.py:860\u001b[39m, in \u001b[36m_ToolNode._execute_tool_sync\u001b[39m\u001b[34m(self, request, input_type, config)\u001b[39m\n\u001b[32m    857\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Error is handled - create error ToolMessage\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m     content = \u001b[43m_handle_tool_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_tool_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ToolMessage(\n\u001b[32m    862\u001b[39m         content=content,\n\u001b[32m    863\u001b[39m         name=call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    864\u001b[39m         tool_call_id=call[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    865\u001b[39m         status=\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    866\u001b[39m     )\n\u001b[32m    868\u001b[39m \u001b[38;5;66;03m# Process successful response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain/tools/tool_node.py:389\u001b[39m, in \u001b[36m_handle_tool_error\u001b[39m\u001b[34m(e, flag)\u001b[39m\n\u001b[32m    387\u001b[39m     content = flag\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(flag):\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     content = \u001b[43mflag\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [assignment, call-arg]\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    391\u001b[39m     msg = (\n\u001b[32m    392\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot unexpected type of `handle_tool_error`. Expected bool, str \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    393\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mor callable. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    394\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain/tools/tool_node.py:352\u001b[39m, in \u001b[36m_default_handle_tool_errors\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ToolInvocationError):\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m e.message\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain/tools/tool_node.py:815\u001b[39m, in \u001b[36m_ToolNode._execute_tool_sync\u001b[39m\u001b[34m(self, request, input_type, config)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    814\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m         response = \u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    816\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    817\u001b[39m         \u001b[38;5;66;03m# Filter out errors for injected arguments\u001b[39;00m\n\u001b[32m    818\u001b[39m         filtered_errors = _filter_validation_errors(\n\u001b[32m    819\u001b[39m             exc,\n\u001b[32m    820\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_state_args.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m], {}),\n\u001b[32m    821\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_store_arg.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    822\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_runtime_arg.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    823\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:591\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    585\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    588\u001b[39m     **kwargs: Any,\n\u001b[32m    589\u001b[39m ) -> Any:\n\u001b[32m    590\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:856\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    854\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    855\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m856\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    857\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    858\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:825\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    824\u001b[39m         tool_kwargs |= {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/github/00-LangGraph-Tutorial/langgraph-v1-tutorial/.venv/lib/python3.11/site-packages/langchain_core/tools/structured.py:90\u001b[39m, in \u001b[36mStructuredTool._run\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.func):\n\u001b[32m     89\u001b[39m         kwargs[config_param] = config\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mStructuredTool does not support sync invocation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mprocess_data\u001b[39m\u001b[34m(data_size, runtime)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;129m@tool\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_data\u001b[39m(data_size: \u001b[38;5;28mint\u001b[39m, runtime: ToolRuntime) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Process data with progress updates.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     writer = \u001b[43mruntime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_stream_writer\u001b[49m()\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# 진행 상황을 커스텀 업데이트로 전송\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, data_size, \u001b[32m10\u001b[39m):\n",
      "\u001b[31mAttributeError\u001b[39m: 'ToolRuntime' object has no attribute 'get_stream_writer'",
      "During task with name 'tools' and id '2e5411ab-8563-51e9-4a67-ff728be7e03e'"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def process_data(data_size: int, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Process data with progress updates.\"\"\"\n",
    "    writer = runtime.get_stream_writer()\n",
    "\n",
    "    # 진행 상황을 커스텀 업데이트로 전송\n",
    "    for i in range(0, data_size, 10):\n",
    "        progress = min(i + 10, data_size)\n",
    "        writer({\"progress\": progress, \"total\": data_size})\n",
    "\n",
    "    return f\"Processed {data_size} items successfully!\"\n",
    "\n",
    "agent = create_agent(model=model, tools=[process_data])\n",
    "\n",
    "# 커스텀 스트리밍 모드로 진행 상황 추적\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Process 50 items of data\"}]},\n",
    "    stream_mode=\"custom\"  # 커스텀 업데이트 수신\n",
    "):\n",
    "    if \"progress\" in chunk:\n",
    "        percentage = (chunk[\"progress\"] / chunk[\"total\"]) * 100\n",
    "        print(f\"Progress: {chunk['progress']}/{chunk['total']} ({percentage:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 다중 스트리밍 모드\n\n여러 스트리밍 모드를 동시에 사용할 수 있습니다. `stream_mode`에 리스트로 전달하면 각 모드의 업데이트를 모두 받을 수 있으며, 반환되는 청크는 `(stream_mode, data)` 튜플 형태입니다.\n\n이 방식은 진행 상황(updates)과 세부 작업 정보(custom)를 동시에 추적해야 할 때 유용합니다.\n\n아래 코드는 여러 스트리밍 모드를 동시에 사용하는 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 스트리밍 모드 동시 사용\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Process 30 items\"}]},\n",
    "    stream_mode=[\"updates\", \"custom\"]  # 여러 모드 동시 사용\n",
    "):\n",
    "    # chunk는 (stream_mode, data) 튜플 형태\n",
    "    mode, data = chunk\n",
    "\n",
    "    if mode == \"updates\":\n",
    "        print(f\"[UPDATE] Node completed: {list(data.keys())}\")\n",
    "    elif mode == \"custom\":\n",
    "        if \"progress\" in data:\n",
    "            print(f\"[PROGRESS] {data['progress']}/{data['total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 스트리밍 비활성화\n\n개별 모델 또는 도구에 대해 스트리밍을 비활성화하려면 해당 객체를 생성할 때 `streaming=False`를 설정합니다. 이 경우 `messages` 모드를 사용해도 토큰이 실시간으로 스트리밍되지 않고 전체 응답이 한 번에 전달됩니다.\n\n스트리밍 비활성화는 응답 전체가 필요한 후처리 작업이나, 네트워크 오버헤드를 줄이고 싶을 때 유용합니다.\n\n아래 코드는 스트리밍을 비활성화한 모델 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 스트리밍 비활성화된 모델\n",
    "non_streaming_model = ChatOpenAI(model=\"gpt-4.1-mini\", streaming=False)\n",
    "\n",
    "agent = create_agent(model=non_streaming_model, tools=[get_weather])\n",
    "\n",
    "# messages 모드를 사용해도 토큰이 스트리밍되지 않음\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    # 전체 응답이 한 번에 전달됨\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 종합 예제: 진행률 바가 있는 데이터 처리\n\n여러 스트리밍 기능을 결합한 실용적인 예제입니다. 데이터 분석 도구에서 단계별 진행 상황을 커스텀 업데이트로 전송하고, 다중 스트리밍 모드로 진행 상황과 최종 결과를 모두 수신합니다.\n\n이 패턴은 대시보드나 모니터링 시스템에서 장시간 실행 작업의 상태를 실시간으로 표시할 때 유용합니다.\n\n아래 코드는 진행률 보고 기능이 포함된 데이터 분석 에이전트 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@tool\n",
    "def analyze_data(dataset_name: str, num_records: int, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Analyze a dataset with detailed progress reporting.\"\"\"\n",
    "    writer = runtime.get_stream_writer()\n",
    "\n",
    "    # 단계별 분석 프로세스\n",
    "    steps = [\n",
    "        (\"loading\", \"Loading data\", 0.2),\n",
    "        (\"cleaning\", \"Cleaning data\", 0.3),\n",
    "        (\"processing\", \"Processing data\", 0.3),\n",
    "        (\"finalizing\", \"Finalizing results\", 0.2)\n",
    "    ]\n",
    "\n",
    "    for step_name, step_desc, duration in steps:\n",
    "        writer({\n",
    "            \"step\": step_name,\n",
    "            \"description\": step_desc,\n",
    "            \"status\": \"started\"\n",
    "        })\n",
    "\n",
    "        time.sleep(duration)  # 작업 시뮬레이션\n",
    "\n",
    "        writer({\n",
    "            \"step\": step_name,\n",
    "            \"description\": step_desc,\n",
    "            \"status\": \"completed\"\n",
    "        })\n",
    "\n",
    "    return f\"Successfully analyzed {num_records} records from {dataset_name}!\"\n",
    "\n",
    "@tool\n",
    "def get_summary(analysis_result: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"Generate a summary of the analysis.\"\"\"\n",
    "    return f\"Analysis complete. {analysis_result}\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "agent = create_agent(model=model, tools=[analyze_data, get_summary])\n",
    "\n",
    "print(\"Starting data analysis...\\n\")\n",
    "\n",
    "# 다중 스트리밍 모드로 실행\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Analyze the sales dataset with 1000 records\"}]},\n",
    "    stream_mode=[\"custom\", \"updates\"]\n",
    "):\n",
    "    mode, data = chunk\n",
    "\n",
    "    if mode == \"custom\":\n",
    "        # 커스텀 진행 상황 표시\n",
    "        if \"step\" in data:\n",
    "            status_icon = \"✓\" if data[\"status\"] == \"completed\" else \"→\"\n",
    "            print(f\"{status_icon} {data['description']}... {data['status']}\")\n",
    "\n",
    "    elif mode == \"updates\":\n",
    "        # 노드 완료 정보 (선택적으로 표시)\n",
    "        if \"messages\" in data:\n",
    "            last_msg = data[\"messages\"][-1]\n",
    "            if hasattr(last_msg, \"content\") and last_msg.content:\n",
    "                print(f\"\\n[Result] {last_msg.content}\")\n",
    "\n",
    "print(\"\\nAnalysis finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 실전 팁\n\n### 적절한 스트리밍 모드 선택\n\n상황에 따라 적절한 스트리밍 모드를 선택하면 더 나은 사용자 경험과 성능을 얻을 수 있습니다.\n\n| 상황 | 권장 모드 | 설명 |\n|:---|:---|:---|\n| 채팅 인터페이스 | messages | 실시간 텍스트 출력으로 자연스러운 대화 경험 |\n| 백엔드 작업 모니터링 | updates + custom | 노드 진행과 세부 작업 상태 동시 추적 |\n| 디버깅/로깅 | updates | 에이전트 실행 흐름 분석 |\n\n아래 코드는 각 상황에 적합한 스트리밍 설정 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 인터페이스에 적합한 설정\n",
    "def chat_interface():\n",
    "    for chunk in agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]},\n",
    "        stream_mode=\"messages\"  # 실시간 응답 표시\n",
    "    ):\n",
    "        if hasattr(chunk, 'content'):\n",
    "            yield chunk.content\n",
    "\n",
    "# 백그라운드 작업 모니터링에 적합한 설정\n",
    "def background_task():\n",
    "    for chunk in agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"Process data\"}]},\n",
    "        stream_mode=[\"updates\", \"custom\"]  # 진행 상황 추적\n",
    "    ):\n",
    "        mode, data = chunk\n",
    "        # 진행 상황을 데이터베이스나 로그에 기록\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 에러 처리\n\n스트리밍 중 네트워크 오류, API 제한 초과 등 예외가 발생할 수 있으므로 적절한 에러 처리가 중요합니다. try-except 블록으로 스트리밍 루프를 감싸면 오류 발생 시에도 애플리케이션이 안정적으로 동작합니다.\n\n아래 코드는 스트리밍 에러 처리 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for chunk in agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"Test query\"}]},\n",
    "        stream_mode=\"messages\"\n",
    "    ):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during streaming: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 성능 최적화\n\n불필요한 스트리밍 모드를 사용하지 않으면 네트워크 오버헤드가 줄어들고 성능이 향상됩니다. 필요한 모드만 선택적으로 사용하는 것이 좋습니다.\n\n아래 코드는 성능 최적화를 위한 스트리밍 설정 예시입니다."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좋은 예: 필요한 모드만 사용\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Query\"}]},\n",
    "    stream_mode=\"messages\"  # 필요한 모드만\n",
    "):\n",
    "    pass\n",
    "\n",
    "# 나쁜 예: 모든 모드 사용 (불필요한 오버헤드)\n",
    "# for chunk in agent.stream(\n",
    "#     {\"messages\": [{\"role\": \"user\", \"content\": \"Query\"}]},\n",
    "#     stream_mode=[\"updates\", \"messages\", \"custom\"]\n",
    "# ):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 정리\n\n이 튜토리얼에서는 LangGraph 에이전트의 스트리밍 기능을 학습했습니다.\n\n**핵심 개념 요약:**\n\n| 스트리밍 모드 | 사용 시점 | 반환 데이터 |\n|:---|:---|:---|\n| **updates** | 노드 실행 진행 상황 추적 | 노드별 출력 메시지 |\n| **messages** | 실시간 텍스트 출력 (채팅 UI) | AIMessageChunk 토큰 |\n| **custom** | 도구의 세부 진행 상황 | 사용자 정의 데이터 |\n\n**주요 기능:**\n- `stream_mode` 매개변수로 스트리밍 모드 선택\n- 리스트로 다중 모드 동시 사용 가능\n- `runtime.stream_writer`로 커스텀 업데이트 전송\n- `streaming=False`로 개별 모델의 스트리밍 비활성화\n\n**다음 단계:**\n- Runtime 컨텍스트를 활용한 고급 도구 구현 학습\n- 구조화된 출력(Structured Output) 사용법 학습",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}