{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph QuickStart\n",
    "\n",
    "LangGraphëŠ” ìƒíƒœ ê¸°ë°˜ ë©€í‹° ì•¡í„° ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "## êµ¬í˜„ ê¸°ëŠ¥\n",
    "\n",
    "- ìƒíƒœ ê´€ë¦¬ ê¸°ë°˜ ì±—ë´‡\n",
    "- ì™¸ë¶€ ë„êµ¬ ì—°ë™ (Tavily Search)\n",
    "- ë©”ëª¨ë¦¬ ë° ì²´í¬í¬ì¸íŠ¸\n",
    "- Human-in-the-Loop\n",
    "- ìƒíƒœ ì»¤ìŠ¤í„°ë§ˆì´ì§•\n",
    "- ìƒíƒœ ì´ë ¥ ê´€ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## í™˜ê²½ ì„¤ì •\n\nLangGraphë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ì™€ ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤. `.env` íŒŒì¼ì— API í‚¤ë¥¼ ì €ì¥í•˜ê³ , LangSmithë¥¼ í†µí•´ ì‹¤í–‰ ê³¼ì •ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê³  LangSmith í”„ë¡œì íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv(override=True)\n",
    "# ì¶”ì ì„ ìœ„í•œ í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •\n",
    "logging.langsmith(\"LangChain-V1-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ê¸°ë³¸ ì±—ë´‡ êµ¬ì¶•\n\në©”ì‹œì§€ ê¸°ë°˜ ì±—ë´‡ì„ StateGraphë¡œ êµ¬ì„±í•©ë‹ˆë‹¤. StateGraphëŠ” LangGraphì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ, ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì—°ê²°í•˜ì—¬ ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n\n> ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Graph API](https://docs.langchain.com/oss/python/langgraph/graph-api.md)\n\n### êµ¬ì„± ìš”ì†Œ\n\n| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |\n|----------|------|\n| StateGraph | ì „ì²´ ì›Œí¬í”Œë¡œìš° íë¦„ì„ ì •ì˜í•˜ëŠ” ê·¸ë˜í”„ |\n| State | ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ìƒíƒœ ê°ì²´ |\n| Node | ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ì˜ˆ: LLM í˜¸ì¶œ) |\n| Edge | ë…¸ë“œ ê°„ ì‹¤í–‰ ê²½ë¡œë¥¼ ì—°ê²° |\n| Compile/Invoke | ê·¸ë˜í”„ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜ ë° í˜¸ì¶œ |\n\nì•„ë˜ ì½”ë“œëŠ” State íƒ€ì…ì„ ì •ì˜í•˜ê³  StateGraph ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# State ì •ì˜: ì±—ë´‡ì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” íƒ€ì…\n",
    "class State(TypedDict):\n",
    "    \"\"\"ì±—ë´‡ì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” íƒ€ì…\n",
    "\n",
    "    messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸\n",
    "    - add_messages í•¨ìˆ˜ë¥¼ í†µí•´ ìƒˆ ë©”ì‹œì§€ê°€ ì¶”ê°€ë¨ (ë®ì–´ì“°ê¸°ê°€ ì•„ë‹Œ ì¶”ê°€)\n",
    "    \"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# StateGraph ìƒì„±\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "print(\"StateGraph ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"StateëŠ” messages í‚¤ë¥¼ ê°€ì§€ë©°, add_messages ë¦¬ë“€ì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### LLM ì„¤ì •\n\nLangGraphì˜ ë…¸ë“œì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤. `init_chat_model` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ì–‘í•œ ì œê³µì(OpenAI, Anthropic ë“±)ì˜ ëª¨ë¸ì„ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” Anthropicì˜ Claude Sonnet 4.5 ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain.chat_models import init_chat_model\n\n# ëª¨ë¸ ì‹ë³„ì ë¬¸ìì—´ì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë°©ë²•\nllm = init_chat_model(\"anthropic:claude-sonnet-4-5\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ì±—ë´‡ ë…¸ë“œ ì¶”ê°€\n\në…¸ë“œëŠ” ê·¸ë˜í”„ì—ì„œ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì±—ë´‡ ë…¸ë“œëŠ” í˜„ì¬ ìƒíƒœì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ LLMì— ì „ë‹¬í•˜ê³ , ì‘ë‹µì„ ìƒˆ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n\n`add_node` ë©”ì„œë“œì˜ ì²« ë²ˆì§¸ ì¸ìëŠ” ë…¸ë“œì˜ ê³ ìœ  ì´ë¦„ì´ê³ , ë‘ ë²ˆì§¸ ì¸ìëŠ” í˜¸ì¶œë  í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì±—ë´‡ ë…¸ë“œ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ê·¸ë˜í”„ì— ì¶”ê°€í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    \"\"\"ì±—ë´‡ ë…¸ë“œ í•¨ìˆ˜\n",
    "\n",
    "    í˜„ì¬ ìƒíƒœì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ LLMì— ì „ë‹¬í•˜ê³ ,\n",
    "    ì‘ë‹µì„ ìƒˆ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "\n",
    "    # ì‘ë‹µì„ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì—¬ ë°˜í™˜\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€\n",
    "# ì²« ë²ˆì§¸ ì¸ì: ë…¸ë“œì˜ ê³ ìœ  ì´ë¦„\n",
    "# ë‘ ë²ˆì§¸ ì¸ì: ë…¸ë“œê°€ ì‚¬ìš©ë  ë•Œ í˜¸ì¶œë  í•¨ìˆ˜\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ì—£ì§€ ì„¤ì •\n\nì—£ì§€ëŠ” ë…¸ë“œ ê°„ì˜ ì‹¤í–‰ íë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤. `START`ëŠ” ê·¸ë˜í”„ ì‹¤í–‰ì˜ ì‹œì‘ì ì´ê³ , `END`ëŠ” ì¢…ë£Œì ì…ë‹ˆë‹¤. ëª¨ë“  ê·¸ë˜í”„ëŠ” ë°˜ë“œì‹œ ì‹œì‘ì ê³¼ ì¢…ë£Œì ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì‹¤í–‰ ê²½ë¡œ(START â†’ chatbot â†’ END)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§„ì…ì : ê·¸ë˜í”„ ì‹¤í–‰ì´ ì‹œì‘ë˜ëŠ” ì§€ì \n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# ì¢…ë£Œì : ê·¸ë˜í”„ ì‹¤í–‰ì´ ëë‚˜ëŠ” ì§€ì \n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "print(\"ì§„ì…ì ê³¼ ì¢…ë£Œì  ì„¤ì • ì™„ë£Œ!\")\n",
    "print(\"ì‹¤í–‰ íë¦„: START â†’ chatbot â†’ END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ê·¸ë˜í”„ ì»´íŒŒì¼\n\nStateGraphë¥¼ ì •ì˜í•œ í›„ì—ëŠ” ë°˜ë“œì‹œ `compile()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì»´íŒŒì¼ ê³¼ì •ì—ì„œ ë…¸ë“œ ê°„ ì—°ê²°ì´ ê²€ì¦ë˜ê³ , ì‹¤í–‰ ìˆœì„œê°€ ê²°ì •ë©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì •ì˜í•œ ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ê·¸ë˜í”„ ì‹œê°í™”\n\nì»´íŒŒì¼ëœ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `langchain_teddynote` íŒ¨í‚¤ì§€ì˜ `visualize_graph` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë…¸ë“œì™€ ì—£ì§€ì˜ ì—°ê²° ìƒíƒœë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì»´íŒŒì¼ëœ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ê·¸ë˜í”„ ì‹¤í–‰\n\n`stream_graph` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ê·¸ë˜í”„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `RunnableConfig`ë¥¼ í†µí•´ ì¬ê·€ ê¹Šì´ ì œí•œ(`recursion_limit`)ê³¼ ìŠ¤ë ˆë“œ ì‹ë³„ì(`thread_id`)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” Configë¥¼ ì„¤ì •í•˜ê³  ì‚¬ìš©ì ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Config ì„¤ì •(recursion_limit: ì¬ê·€ ê¹Šì´ ì œí•œ, thread_id: ìŠ¤ë ˆë“œ ì•„ì´ë””)\n",
    "config = RunnableConfig(recursion_limit=20, thread_id=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”! LangGraphì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\")]\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¬ë°\n",
    "stream_graph(graph, inputs=inputs, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ë„êµ¬(Tools) ì¶”ê°€\n\nì™¸ë¶€ ê²€ìƒ‰ ë„êµ¬ë¥¼ í†µí•©í•˜ì—¬ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ì±—ë´‡ì„ ë§Œë“­ë‹ˆë‹¤. LangGraphëŠ” LLMì´ ì™¸ë¶€ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³  ê·¸ ê²°ê³¼ë¥¼ í™œìš©í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì‰½ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n> ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Tool Integration](https://docs.langchain.com/oss/python/langgraph/tool-integration.md)\n\n### í•µì‹¬ ê°œë…\n\n| ê°œë… | ì„¤ëª… |\n|------|------|\n| Tool Binding | LLMì— ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ì—°ê²°í•˜ëŠ” ê³¼ì • |\n| Tool Node | ì‹¤ì œ ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë…¸ë“œ |\n| Conditional Edges | LLM ì‘ë‹µì— ë”°ë¼ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ìë™ ë¶„ê¸° |\n\nì•„ë˜ ì½”ë“œëŠ” ê°„ë‹¨í•œ ë§ì…ˆ ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³µìœ \n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int):\n",
    "    \"ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤.\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Tavily ê²€ìƒ‰ ë„êµ¬ ì„¤ì •\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool, add]\n",
    "\n",
    "# ë„êµ¬ í…ŒìŠ¤íŠ¸\n",
    "result = tool.invoke(\"LangGraphë€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(f\"ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(result['results'])}ê°œ\")\n",
    "print(f\"ì²« ë²ˆì§¸ ê²°ê³¼ ì œëª©: {result['results'][0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ë„êµ¬ ì‚¬ìš© ê·¸ë˜í”„ êµ¬ì„±\n\nê¸°ë³¸ ì±—ë´‡ íë¦„ì— ë„êµ¬ í˜¸ì¶œ ê²½ë¡œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. LLMì´ ë„êµ¬ í˜¸ì¶œì„ ìš”ì²­í•˜ë©´ \"tools\" ë…¸ë“œë¡œ ì´ë™í•˜ê³ , ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì±—ë´‡ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ìˆœí™˜ êµ¬ì¡°(chatbot â‡„ tools)ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” LLMì— ë„êµ¬ë¥¼ ë°”ì¸ë”©í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret1 = llm_with_tools.invoke(\"LangGraph ê°€ ë­ì•¼?\")\n",
    "ret2 = llm_with_tools.invoke(\"LangGraph ê°€ ë­ì•¼? ê²€ìƒ‰í•´ì„œ ì•Œë ¤ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import display_message_tree\n",
    "\n",
    "display_message_tree(ret1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_message_tree(ret2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# State ì •ì˜ (ë™ì¼)\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# ìƒˆë¡œìš´ ê·¸ë˜í”„ ë¹Œë” ìƒì„±\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# LLMì— ë„êµ¬ ë°”ì¸ë”© - LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    \"\"\"ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì±—ë´‡ ë…¸ë“œ\"\"\"\n",
    "    # ë„êµ¬ê°€ ë°”ì¸ë”©ëœ LLM í˜¸ì¶œ\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# ToolNode ì¶”ê°€ - ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ\n",
    "tool_node = ToolNode(tools=tools)\n",
    "builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¡°ê±´ë¶€ ë¼ìš°íŒ…\n",
    "\n",
    "`tools_condition`ì´ ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ì˜ `tool_calls` ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•´ ê²½ë¡œë¥¼ ë¶„ê¸°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools_condition ë™ì‘\n",
    "\n",
    "`tool_calls` ì¡´ì¬ ì‹œ \"tools\"ë¡œ, ì—†ìœ¼ë©´ \"\\_\\_end\\_\\_\"ë¡œ ë¶„ê¸°í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "def tools_condition(state) -> Literal[\"tools\", \"__end__\"]:\n",
    "    ai_message = state[-1] if isinstance(state, list) else state[\"messages\"][-1]\n",
    "    return \"tools\" if getattr(ai_message, \"tool_calls\", []) else \"__end__\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "# tools_conditionì€ ë©”ì‹œì§€ì— tool_callsê°€ ìˆìœ¼ë©´ \"tools\"ë¡œ,\n",
    "# ì—†ìœ¼ë©´ ENDë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤\n",
    "builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,  # ì‚¬ì „ ì •ì˜ëœ ì¡°ê±´ í•¨ìˆ˜ ì‚¬ìš©\n",
    ")\n",
    "# Literal[\"tools\", \"__end__\"]\n",
    "\n",
    "# ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì±—ë´‡ìœ¼ë¡œ ëŒì•„ê°€ê¸°\n",
    "builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# ì‹œì‘ì  ì„¤ì •\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_with_tools = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ê·¸ë˜í”„ ì‹œê°í™”\n\në„êµ¬ê°€ ì—°ê²°ëœ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. chatbot ë…¸ë“œì—ì„œ ì¡°ê±´ì— ë”°ë¼ tools ë…¸ë“œë¡œ ë¶„ê¸°í•˜ëŠ” êµ¬ì¡°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ë„êµ¬ê°€ ì—°ê²°ëœ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(graph_with_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ë„êµ¬ ì‚¬ìš© í…ŒìŠ¤íŠ¸\n\nTavily ê²€ìƒ‰ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. LLMì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í•˜ë©´ ìë™ìœ¼ë¡œ ê²€ìƒ‰ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” 2025ë…„ LangGraph ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "stream_graph(\n",
    "    graph_with_tools,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"2025ë…„ LangGraph ì‚¬ìš© ì‚¬ë¡€ ì•Œë ¤ì£¼ì„¸ìš”.\")]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ë©”ëª¨ë¦¬ ì¶”ê°€\n\nì„¸ì…˜ ê°„ ì‚¬ìš©ì ì •ë³´ë¥¼ ìœ ì§€í•˜ëŠ” ì˜êµ¬ ìƒíƒœ ê´€ë¦¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ í†µí•´ ì±—ë´‡ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³ , ì‚¬ìš©ìë³„ë¡œ ê°œì¸í™”ëœ ì‘ë‹µì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n> ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Persistence](https://docs.langchain.com/oss/python/langgraph/persistence.md)\n\n### í•µì‹¬ ê°œë…\n\n| ê°œë… | ì„¤ëª… |\n|------|------|\n| Checkpointer | ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ë³µì›í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ |\n| Thread ID | ë™ì¼ ì„¸ì…˜ì„ ì‹ë³„í•˜ëŠ” ê³ ìœ  ì‹ë³„ì |\n| User ID | ì‚¬ìš©ìë³„ ì¥ê¸° ê¸°ì–µì„ ê´€ë¦¬í•˜ëŠ” ì‹ë³„ì |\n| Persistent State | ëˆ„ì ëœ ëŒ€í™” ì´ë ¥ ê¸°ë°˜ì˜ ì»¨í…ìŠ¤íŠ¸ |\n\nì•„ë˜ ì½”ë“œëŠ” ë©”ëª¨ë¦¬ ì¶”ì¶œì„ ìœ„í•œ Pydantic ëª¨ë¸ê³¼ ì¶”ì¶œê¸°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import List, Optional\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import PydanticOutputParser\nimport os\n\n\n# Pydantic ëª¨ë¸ ì •ì˜\nclass MemoryItem(BaseModel):\n    \"\"\"ê°œë³„ ë©”ëª¨ë¦¬ ì•„ì´í…œ\"\"\"\n\n    key: str = Field(description=\"ë©”ëª¨ë¦¬ í‚¤ (ì˜ˆ: user_name, preference, fact)\")\n    value: str = Field(description=\"ë©”ëª¨ë¦¬ ê°’\")\n    category: str = Field(\n        description=\"ì¹´í…Œê³ ë¦¬ (personal_info, preference, interest, relationship, fact, etc.)\"\n    )\n    importance: int = Field(description=\"ì¤‘ìš”ë„ (1-5, 5ê°€ ê°€ì¥ ì¤‘ìš”)\", ge=1, le=5)\n    confidence: float = Field(description=\"ì¶”ì¶œ ì‹ ë¢°ë„ (0.0-1.0)\", ge=0.0, le=1.0)\n\n\nclass ExtractedMemories(BaseModel):\n    \"\"\"ì¶”ì¶œëœ ë©”ëª¨ë¦¬ ì»¬ë ‰ì…˜\"\"\"\n\n    memories: List[MemoryItem] = Field(description=\"ì¶”ì¶œëœ ë©”ëª¨ë¦¬ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸\")\n    summary: str = Field(description=\"ëŒ€í™” ë‚´ìš© ìš”ì•½\")\n    timestamp: str = Field(\n        default_factory=lambda: datetime.now().isoformat(), description=\"ì¶”ì¶œ ì‹œê°„\"\n    )\n\n\n# ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\nDEFAULT_SYSTEM_PROMPT = \"\"\"You are an expert memory extraction assistant. Your task is to extract important information from user conversations and convert them into structured key-value pairs for long-term memory storage.\n\nExtract ALL relevant information from the conversation, including:\n- Personal information (name, age, location, occupation, etc.)\n- Preferences and interests\n- Relationships and social connections\n- Important facts or events mentioned\n- Opinions and beliefs\n- Goals and aspirations\n- Any other notable information\n\nFor each piece of information:\n1. Create a concise, searchable key\n2. Store the complete value\n3. Categorize appropriately\n4. Assess importance (1-5 scale)\n5. Evaluate extraction confidence (0.0-1.0)\"\"\"\n\n\ndef create_memory_extractor(\n    model_name: Optional[str] = \"anthropic:claude-sonnet-4-5\",\n    system_prompt: Optional[str] = None,\n) -> any:\n    \"\"\"ë©”ëª¨ë¦¬ ì¶”ì¶œê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n\n    Args:\n        model: ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸. Noneì¼ ê²½ìš° ê¸°ë³¸ ChatOpenAI ëª¨ë¸ ì‚¬ìš©\n        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸. Noneì¼ ê²½ìš° ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n\n    Returns:\n        ë©”ëª¨ë¦¬ ì¶”ì¶œ ì²´ì¸\n    \"\"\"\n    # Output Parser ìƒì„±\n    memory_parser = PydanticOutputParser(pydantic_object=ExtractedMemories)\n\n    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •\n    if system_prompt is None:\n        system_prompt = DEFAULT_SYSTEM_PROMPT\n\n    # ì „ì²´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±\n    template = f\"\"\"{system_prompt}\n\nUser Input: {{input}}\n\n{{format_instructions}}\n\nRemember to:\n- Extract multiple memory items if the conversation contains various pieces of information\n- Use clear, consistent key naming conventions\n- Preserve context in values when necessary\n- Be comprehensive but avoid redundancy\n\"\"\"\n\n    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n    prompt = ChatPromptTemplate.from_template(\n        template,\n        partial_variables={\n            \"format_instructions\": memory_parser.get_format_instructions()\n        },\n    )\n\n    # ëª¨ë¸ ì„¤ì •\n    model = init_chat_model(model_name)\n\n    # ë©”ëª¨ë¦¬ ì¶”ì¶œ ì²´ì¸ ìƒì„±\n    memory_extractor = prompt | model | memory_parser\n\n    return memory_extractor"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import Any\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.store.base import BaseStore\nfrom langchain_openai import ChatOpenAI\n\nfrom langchain_teddynote.memory import create_memory_extractor\nimport uuid\n\nmodel = init_chat_model(\"anthropic:claude-sonnet-4-5\")\nmemory_extractor = create_memory_extractor(model=\"anthropic:claude-sonnet-4-5\")\n\n\ndef call_model(\n    state: MessagesState,\n    config: RunnableConfig,\n    *,\n    store: BaseStore,\n) -> dict[str, Any]:\n    \"\"\"LLM ëª¨ë¸ì„ í˜¸ì¶œí•˜ê³  ì‚¬ìš©ì ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\n\n    Args:\n        state (MessagesState): ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ëŠ” í˜„ì¬ ìƒíƒœ\n        config (RunnableConfig): ì‹¤í–‰ ê°€ëŠ¥ êµ¬ì„±\n        store (BaseStore): ë©”ëª¨ë¦¬ ì €ì¥ì†Œ\n    \"\"\"\n    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œ user_id ì¶”ì¶œ\n    user_id = config[\"configurable\"][\"user_id\"]\n    namespace = (\"memories\", user_id)\n\n    print(namespace)\n\n    # ìœ ì €ì˜ ë©”ëª¨ë¦¬ ê²€ìƒ‰\n    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n    info = \"\\n\".join([f\"{memory.key}: {memory.value}\" for memory in memories])\n    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n    # ì‚¬ìš©ìê°€ ê¸°ì–µ ìš”ì²­ ì‹œ ë©”ëª¨ë¦¬ ì €ì¥\n    last_message = state[\"messages\"][-1]\n    if \"remember\" in last_message.content.lower():\n        result = memory_extractor.invoke({\"input\": str(state[\"messages\"][-1].content)})\n        for memory in result.memories:\n            print(memory)\n            print(\"-\" * 100)\n            store.put(namespace, str(uuid.uuid4()), {memory.key: memory.value})\n\n    # LLM í˜¸ì¶œ\n    response = model.invoke(\n        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n    )\n    return {\"messages\": response}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# ê·¸ë˜í”„ ë¹Œë“œ\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ìƒì„±\n",
    "# ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” PostgresSaver ì‚¬ìš© ê¶Œì¥\n",
    "memory_saver = InMemorySaver()\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_with_memory = builder.compile(\n",
    "    checkpointer=memory_saver,\n",
    "    store=memory_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "\n",
    "def run_graph(\n",
    "    msg,\n",
    "    thread_id=\"default\",\n",
    "    user_id=\"default\",\n",
    "):\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id + user_id,\n",
    "            \"user_id\": user_id,\n",
    "        }\n",
    "    }\n",
    "    print(f\"\\n[ìœ ì €] {msg}\")\n",
    "    stream_graph(\n",
    "        graph_with_memory,\n",
    "        inputs={\"messages\": [{\"role\": \"user\", \"content\": msg}]},\n",
    "        config=config,\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ì•ˆë…•? ë‚´ ì´ë¦„ì€ í…Œë””ì•¼\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¥ê¸° ê¸°ì–µ ì €ì¥\n",
    "\n",
    "ë©”ì‹œì§€ì— `remember` í‚¤ì›Œë“œ í¬í•¨ ì‹œ ì¥ê¸° ì €ì¥ì†Œì— ì •ë³´ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ í…Œë””ì•¼ remember\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread ê°„ ì§€ì†ì„±\n",
    "\n",
    "User ID ê¸°ë°˜ ì¥ê¸° ê¸°ì–µì€ Threadê°€ ë‹¬ë¼ë„ ìœ ì§€ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆë”ë¼?\", \"1004\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\n",
    "    \"ë‚´ ì§ì—…ì€ AI Engineer ì•¼. ë‚´ ì·¨ë¯¸ëŠ” Netflix ë³´ê¸° ì•¼. remember\", \"4\", \"someone\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰\n",
    "run_graph(\"ë‚´ ì´ë¦„, ì§ì—…, ì·¨ë¯¸ ì•Œë ¤ì¤˜\", \"100\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ user_id ë¡œ ì‹¤í–‰í•œ ê²½ìš°\n",
    "run_graph(\"ë‚´ ì´ë¦„, ì§ì—…, ì·¨ë¯¸ ì•Œë ¤ì¤˜\", \"100\", \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### State í™•ì¸\n\nì €ì¥ëœ ìƒíƒœë¥¼ ì¡°íšŒí•˜ì—¬ ë©”ì‹œì§€ ì´ë ¥ê³¼ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. `get_state` ë©”ì„œë“œë¥¼ í†µí•´ í˜„ì¬ ìƒíƒœì˜ ìŠ¤ëƒ…ìƒ·ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” íŠ¹ì • Configì— ëŒ€í•œ í˜„ì¬ ìƒíƒœ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ì˜ì˜ Config ì„¤ì •\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"100\" + \"someone\",\n",
    "        \"user_id\": \"someone\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "snapshot = graph_with_memory.get_state(config)\n",
    "\n",
    "print(\"í˜„ì¬ ìƒíƒœ ì •ë³´:\")\n",
    "print(f\"- ë©”ì‹œì§€ ìˆ˜: {len(snapshot.values['messages'])}ê°œ\")\n",
    "print(f\"- ì²´í¬í¬ì¸íŠ¸ ID: {snapshot.config['configurable']['checkpoint_id']}\")\n",
    "\n",
    "# ìµœê·¼ ë©”ì‹œì§€ ëª‡ ê°œ í‘œì‹œ\n",
    "print(\"\\n[ìµœê·¼ ë©”ì‹œì§€]\")\n",
    "for msg in snapshot.values[\"messages\"]:\n",
    "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
    "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
    "    print(f\"  [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Human-in-the-Loop\n\nê³ ìœ„í—˜ ì‘ì—…ì— ëŒ€í•´ ì¸ê°„ ìŠ¹ì¸ì„ ìš”ì²­í•˜ëŠ” íë¦„ì„ ë„ì…í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²°ì •ì´ë‚˜ ë¯¼ê°í•œ ì‘ì—… ìˆ˜í–‰ ì „ì— ì‚¬ëŒì˜ í™•ì¸ì„ ë°›ì„ ìˆ˜ ìˆì–´ AI ì‹œìŠ¤í…œì˜ ì•ˆì „ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n\n> ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Interrupts](https://docs.langchain.com/oss/python/langgraph/interrupts.md)\n\n### í•µì‹¬ ê°œë…\n\n| ê°œë… | ì„¤ëª… |\n|------|------|\n| interrupt | ê·¸ë˜í”„ ì‹¤í–‰ì„ ì¼ì‹œì •ì§€í•˜ê³  ì™¸ë¶€ ì…ë ¥ì„ ëŒ€ê¸° |\n| Command | ìŠ¹ì¸/ê±°ë¶€ í›„ ì¬ê°œ ëª…ë ¹ì„ ì „ë‹¬í•˜ëŠ” ê°ì²´ |\n| Human Approval | ì‚¬ëŒì´ ê²€í† í•˜ê³  ìŠ¹ì¸í•˜ëŠ” ì›Œí¬í”Œë¡œìš° |\n\nì•„ë˜ ì½”ë“œëŠ” ì‚¬ëŒì˜ ë„ì›€ì„ ìš”ì²­í•˜ëŠ” ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from an expert(human).\"\"\"\n",
    "    # interruptë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰ ì¼ì‹œ ì¤‘ì§€\n",
    "    # ì‚¬ëŒì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¼\n",
    "    human_response = interrupt({\"query\": query})\n",
    "\n",
    "    # ì‚¬ëŒì˜ ì‘ë‹µ ë°˜í™˜\n",
    "    return human_response[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### HITL ê·¸ë˜í”„ êµ¬ì„±\n\n`human_assistance` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ë„êµ¬ê°€ í˜¸ì¶œë˜ë©´ `interrupt`ë¡œ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ê³ , ì‚¬ëŒì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. ì‘ë‹µì´ ì…ë ¥ë˜ë©´ í•´ë‹¹ ë‚´ìš©ì„ ë°˜í™˜í•˜ì—¬ ëŒ€í™”ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” Human-in-the-Loop ê¸°ëŠ¥ì´ í¬í•¨ëœ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸\n",
    "tools_with_human = [human_assistance]\n",
    "\n",
    "# ìƒˆë¡œìš´ ê·¸ë˜í”„ êµ¬ì„±\n",
    "graph_builder_hitl = StateGraph(State)\n",
    "\n",
    "# LLMì— ë„êµ¬ ë°”ì¸ë”©\n",
    "llm_with_human_tools = llm.bind_tools(tools_with_human)\n",
    "\n",
    "\n",
    "def chatbot_with_human(state: State):\n",
    "    \"\"\"Human Interuption ìš”ì²­í•  ìˆ˜ ìˆëŠ” ì±—ë´‡\"\"\"\n",
    "    message = llm_with_human_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    # interrupt ì¤‘ ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ ë°©ì§€\n",
    "    # (ì¬ê°œ ì‹œ ë„êµ¬ í˜¸ì¶œì´ ë°˜ë³µë˜ëŠ” ê²ƒì„ ë°©ì§€)\n",
    "    if hasattr(message, \"tool_calls\"):\n",
    "        assert (\n",
    "            len(message.tool_calls) <= 1\n",
    "        ), \"ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œì€ interruptì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
    "\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "graph_builder_hitl.add_node(\"chatbot_with_human\", chatbot_with_human)\n",
    "\n",
    "# ToolNode ì¶”ê°€\n",
    "tool_node_hitl = ToolNode(tools=tools_with_human)\n",
    "graph_builder_hitl.add_node(\"tools\", tool_node_hitl)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "graph_builder_hitl.add_conditional_edges(\"chatbot_with_human\", tools_condition)\n",
    "graph_builder_hitl.add_edge(\"tools\", \"chatbot_with_human\")\n",
    "graph_builder_hitl.add_edge(START, \"chatbot_with_human\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼\n",
    "memory_hitl = InMemorySaver()\n",
    "graph_hitl = graph_builder_hitl.compile(checkpointer=memory_hitl)\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(graph_hitl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### HITL í…ŒìŠ¤íŠ¸\n\nì‚¬ëŒì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ interruptì™€ ì¬ê°œ íë¦„ì„ ê²€ì¦í•©ë‹ˆë‹¤. ê·¸ë˜í”„ê°€ ì¤‘ë‹¨ë˜ë©´ `get_state`ë¡œ í˜„ì¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , `Command(resume=...)`ë¡œ ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì¸ê°„ ì§€ì›ì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€ë¡œ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import random_uuid\n",
    "\n",
    "# ì¸ê°„ ì§€ì›ì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€\n",
    "user_input = \"LangGraph ê°€ ë­ì•¼? ì‚¬ëŒí•œí…Œ ë“£ê³  ì‹¶ì–´.\"\n",
    "config_hitl = {\"configurable\": {\"thread_id\": random_uuid()}}\n",
    "\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "stream_graph(\n",
    "    graph_hitl,\n",
    "    inputs={\"messages\": [HumanMessage(content=user_input)]},\n",
    "    config=config_hitl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ í™•ì¸ - ì–´ëŠ ë…¸ë“œì—ì„œ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "snapshot = graph_hitl.get_state(config_hitl)\n",
    "print(f\"\\ní˜„ì¬ ìƒíƒœ:\")\n",
    "print(f\"  ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ: {snapshot.next}\")\n",
    "print(f\"  ì²´í¬í¬ì¸íŠ¸ ID: {snapshot.config['configurable']['checkpoint_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ê°„ì˜ ì‘ë‹µìœ¼ë¡œ ì‹¤í–‰ ì¬ê°œ\n",
    "human_response = \"\"\"## ì „ë¬¸ê°€ì˜ ì¡°ì–¸:\n",
    "- YouTube í…Œë””ë…¸íŠ¸: https://www.youtube.com/c/teddynote\n",
    "- ê³ ê¸‰ ê°œë°œì ê°•ì˜ [íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ RAG ë¹„ë²•ë…¸íŠ¸](https://fastcampus.co.kr/data_online_teddy)\n",
    "\"\"\"\n",
    "\n",
    "# Command ê°ì²´ë¡œ ì¬ê°œ\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "print(f\"\\nì‚¬ëŒì˜ ì‘ë‹µ: {human_response}\\n\")\n",
    "\n",
    "# ì¬ê°œ\n",
    "stream_graph(graph_hitl, inputs=human_command, config=config_hitl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ìƒíƒœ ì»¤ìŠ¤í„°ë§ˆì´ì§•\n\në©”ì‹œì§€ ì™¸ì— ì—…ë¬´ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì»¤ìŠ¤í…€ ìƒíƒœì™€ ë„êµ¬ ê¸°ë°˜ ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ë„ì…í•©ë‹ˆë‹¤. `Command` ê°ì²´ë¥¼ ì‚¬ìš©í•˜ë©´ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¡œ ìƒíƒœë¥¼ ì§ì ‘ ê°±ì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### í•µì‹¬ ê°œë…\n\n| ê°œë… | ì„¤ëª… |\n|------|------|\n| Custom State Fields | messages ì™¸ì— ì¶”ê°€ í•„ë“œë¥¼ ì •ì˜ |\n| State Updates from Tools | ë„êµ¬ ê²°ê³¼ë¡œ ìƒíƒœë¥¼ ì§ì ‘ ê°±ì‹  |\n| Command(update=...) | ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ì§€ì •í•˜ëŠ” ëª…ë ¹ ê°ì²´ |\n\nì•„ë˜ ì½”ë“œëŠ” `human_feedback` í•„ë“œê°€ ì¶”ê°€ëœ ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId\n",
    "\n",
    "\n",
    "# í™•ì¥ëœ State ì •ì˜\n",
    "class CustomState(TypedDict):\n",
    "    \"\"\"ì»¤ìŠ¤í…€ í•„ë“œê°€ ì¶”ê°€ëœ ìƒíƒœ\"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages]\n",
    "    human_feedback: str  # ì‚¬ëŒì˜ í”¼ë“œë°±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ìƒíƒœ ì—…ë°ì´íŠ¸ ë„êµ¬\n\në„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ `Command(update=...)`ë¡œ ìƒíƒœì— ë°˜ì˜í•©ë‹ˆë‹¤. ì´ íŒ¨í„´ì„ ì‚¬ìš©í•˜ë©´ ë„êµ¬ê°€ ë‹¨ìˆœíˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìƒíƒœì˜ íŠ¹ì • í•„ë“œë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì¸ê°„ ê²€í† ë¥¼ ìš”ì²­í•˜ê³  í”¼ë“œë°±ì— ë”°ë¼ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def human_review(\n",
    "    human_feedback, tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Request human review for information.\"\"\"\n",
    "    # ì¸ê°„ì—ê²Œ ê²€í†  ìš”ì²­\n",
    "    human_response = interrupt(\n",
    "        {\"question\": \"ì´ ì •ë³´ê°€ ë§ë‚˜ìš”?\", \"human_feedback\": human_feedback}\n",
    "    )\n",
    "\n",
    "    feedback = human_response.get(\"human_feedback\", \"\")\n",
    "\n",
    "    if feedback.strip() == \"\":\n",
    "        # ì‚¬ìš©ìê°€ AI ì˜ ë‹µë³€ì— ë™ì˜í•˜ëŠ” ê²½ìš°\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [ToolMessage(human_response, tool_call_id=tool_call_id)]\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # ì‚¬ìš©ìê°€ AI ì˜ ë‹µë³€ì— ë™ì˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°\n",
    "        corrected_information = f\"# ì‚¬ìš©ìì— ì˜í•´ ìˆ˜ì •ëœ í”¼ë“œë°±: {feedback}\"\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [\n",
    "                    ToolMessage(corrected_information, tool_call_id=tool_call_id)\n",
    "                ]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ì»¤ìŠ¤í…€ ìƒíƒœ ê·¸ë˜í”„\n\n`CustomState`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ê¸°ë³¸ `State` ëŒ€ì‹  ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ë©´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í•„ìš”í•œ ì¶”ê°€ ë°ì´í„°ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì»¤ìŠ¤í…€ ìƒíƒœì™€ `human_review` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸\n",
    "tools_custom = [human_review]\n",
    "\n",
    "# ìƒˆë¡œìš´ ê·¸ë˜í”„ êµ¬ì„±\n",
    "custom_graph_builder = StateGraph(CustomState)  # CustomState ì‚¬ìš©\n",
    "\n",
    "# LLMì— ë„êµ¬ ë°”ì¸ë”©\n",
    "llm_with_custom_tools = llm.bind_tools(tools_custom)\n",
    "\n",
    "\n",
    "def chatbot_custom(state: CustomState):\n",
    "    \"\"\"ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡\"\"\"\n",
    "    message = llm_with_custom_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    if hasattr(message, \"tool_calls\"):\n",
    "        assert len(message.tool_calls) <= 1\n",
    "\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# ë…¸ë“œì™€ ì—£ì§€ ì¶”ê°€\n",
    "custom_graph_builder.add_node(\"chatbot\", chatbot_custom)\n",
    "tool_node_custom = ToolNode(tools=tools_custom)\n",
    "custom_graph_builder.add_node(\"tools\", tool_node_custom)\n",
    "\n",
    "custom_graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "custom_graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "custom_graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "memory_custom = InMemorySaver()\n",
    "custom_graph = custom_graph_builder.compile(checkpointer=memory_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ê·¸ë˜í”„ ì‹œê°í™”\n\nì»¤ìŠ¤í…€ ìƒíƒœ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. ê¸°ë³¸ ë„êµ¬ ê·¸ë˜í”„ì™€ ë™ì¼í•œ êµ¬ì¡°ì´ì§€ë§Œ, ë‚´ë¶€ì ìœ¼ë¡œ `CustomState`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì»¤ìŠ¤í…€ ìƒíƒœ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(custom_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ì»¤ìŠ¤í…€ ìƒíƒœ í…ŒìŠ¤íŠ¸\n\n`human_review` ë„êµ¬ í˜¸ì¶œ ì‹œ interruptë¡œ ì¤‘ë‹¨ë˜ê³ , ì¬ê°œ ì‹œ ì‚¬ìš©ì í”¼ë“œë°±ì— ë”°ë¼ ìƒíƒœê°€ ê°±ì‹ ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë¹ˆ í”¼ë“œë°±ì„ ì œê³µí•˜ë©´ AIì˜ ë‹µë³€ì— ë™ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì¸ê°„ ê²€í† ê°€ í•„ìš”í•œ ì§ˆë¬¸ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraphì˜ ì¶œì‹œì¼ì„ ì¡°ì‚¬í•˜ê³  ê²€í†  ìš”ì²­\n",
    "user_input = (\n",
    "    \"2024ë…„ ë…¸ë²¨ ë¬¸í•™ìƒ ìˆ˜ìƒìê°€ ëˆ„êµ¬ì¸ì§€ ì¡°ì‚¬í•´ì£¼ì„¸ìš”. \"\n",
    "    \"ë‹µì„ ì°¾ìœ¼ë©´ `human_review` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ê²€í† ë¥¼ ìš”ì²­í•˜ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "custom_config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "# ì‹¤í–‰ (interruptì—ì„œ ì¤‘ë‹¨ë  ê²ƒì„)\n",
    "stream_graph(\n",
    "    custom_graph,\n",
    "    inputs={\"messages\": [HumanMessage(content=user_input)]},\n",
    "    config=custom_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import display_message_tree\n",
    "\n",
    "# ìµœì‹  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "last_message = custom_graph.get_state(custom_config).values[\"messages\"][-1]\n",
    "\n",
    "# ìµœì‹  ë©”ì‹œì§€ tree êµ¬ì¡°ë¡œ í‘œì‹œ\n",
    "display_message_tree(last_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI ê°€ ì‘ì„±í•œ ë‚´ìš©\n",
    "print(last_message.tool_calls[0][\"args\"][\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ê°„ì˜ ê²€í†  ì‘ë‹µìœ¼ë¡œ ì¬ê°œ\n",
    "human_command = Command(\n",
    "    resume={\"human_feedback\": \"2024ë…„ ë…¸ë²¨ ë¬¸í•™ìƒ ìˆ˜ìƒìëŠ” ëŒ€í•œë¯¼êµ­ì˜ í•œê°• ì‘ê°€ì…ë‹ˆë‹¤.\"}\n",
    ")\n",
    "\n",
    "stream_graph(custom_graph, inputs=human_command, config=custom_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ìƒíƒœ ì´ë ¥ ê´€ë¦¬\n\nì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ìƒíƒœë¥¼ ì €ì¥/ë³µì›í•˜ì—¬ ë¡¤ë°±/ì¬ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì„ í†µí•´ íŠ¹ì • ì‹œì ìœ¼ë¡œ ëŒì•„ê°€ ë‹¤ë¥¸ ê²½ë¡œë¡œ ì‹¤í–‰í•˜ê±°ë‚˜, ì‹¤í–‰ ì´ë ¥ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n### í•µì‹¬ ê°œë…\n\n| ê°œë… | ì„¤ëª… |\n|------|------|\n| State History | ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ë°œìƒí•œ ëª¨ë“  ìƒíƒœ ë³€ê²½ ì´ë ¥ |\n| Checkpoint ID | íŠ¹ì • ì‹œì ì˜ ìƒíƒœë¥¼ ì‹ë³„í•˜ëŠ” ê³ ìœ  ID |\n| Rollback | ì§€ì •í•œ ì²´í¬í¬ì¸íŠ¸ë¡œ ìƒíƒœë¥¼ ë³µì› |\n| Resume | ë³µì›ëœ ìƒíƒœì—ì„œ ì‹¤í–‰ì„ ì¬ê°œ |\n\nì•„ë˜ ì„¹ì…˜ì—ì„œëŠ” ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì„±\n\nìƒíƒœ ì´ë ¥ í™•ì¸ê³¼ ë¡¤ë°±/ì¬ì‹¤í–‰ì„ ìœ„í•œ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. Tavily ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ë²ˆì˜ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•˜ê³ , ê° ë‹¨ê³„ì˜ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì²´í¬í¬ì¸í„°ê°€ ì—°ê²°ëœ ê²€ìƒ‰ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ ê´€ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ê·¸ë˜í”„\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# ë„êµ¬ì™€ LLM ì„¤ì •\n",
    "tools = [TavilySearch(max_results=2)]\n",
    "llm_with_tools_tt = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot_tt(state: State):\n",
    "    \"\"\"ìƒíƒœ ê´€ë¦¬ í…ŒìŠ¤íŠ¸ìš© ì±—ë´‡\"\"\"\n",
    "    return {\"messages\": [llm_with_tools_tt.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ êµ¬ì„±\n",
    "graph_builder.add_node(\"chatbot\", chatbot_tt)\n",
    "tool_node_tt = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node_tt)\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼\n",
    "memory_tt = InMemorySaver()\n",
    "time_travel_graph = graph_builder.compile(checkpointer=memory_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ê·¸ë˜í”„ ì‹œê°í™”\n\nì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì´ì „ì— êµ¬ì„±í•œ ë„êµ¬ ê·¸ë˜í”„ì™€ ë™ì¼í•œ êµ¬ì¡°ì…ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™”\n",
    "visualize_graph(time_travel_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ì²´í¬í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ìƒì„±\n\nì—¬ëŸ¬ ë²ˆ ëŒ€í™”ë¥¼ ì‹¤í–‰í•˜ì—¬ ìƒíƒœ ì´ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ê° ì‹¤í–‰ë§ˆë‹¤ ìƒˆë¡œìš´ ì²´í¬í¬ì¸íŠ¸ê°€ ìƒì„±ë˜ì–´ ë‚˜ì¤‘ì— íŠ¹ì • ì‹œì ìœ¼ë¡œ ë¡¤ë°±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì²« ë²ˆì§¸ ê²€ìƒ‰ ëŒ€í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_travel_config = RunnableConfig(configurable={\"thread_id\": \"time-travel-1\"})\n",
    "\n",
    "# ì²« ë²ˆì§¸ ëŒ€í™”\n",
    "stream_graph(\n",
    "    time_travel_graph,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"í…Œë””ë…¸íŠ¸ì— ëŒ€í•´ì„œ ì¡°ì‚¬ ì¢€ í•´ì£¼ì„¸ìš”.\")]},\n",
    "    config=time_travel_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ë²ˆì§¸ ëŒ€í™”\n",
    "stream_graph(\n",
    "    time_travel_graph,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"í…Œë””ë…¸íŠ¸ ì˜¨ë¼ì¸ ê°•ì˜ ì£¼ì†Œë¥¼ ì¡°ì‚¬ í•´ì£¼ì„¸ìš”.\")]\n",
    "    },\n",
    "    config=time_travel_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ìƒíƒœ ì´ë ¥ íƒìƒ‰\n\n`get_state_history`ë¡œ ì „ì²´ ìƒíƒœ ì´ë ¥ì„ ì¡°íšŒí•˜ê³  ë¡¤ë°±í•  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì´ë ¥ì€ ìµœì‹ ìˆœìœ¼ë¡œ ë°˜í™˜ë˜ë©°, ê° ì²´í¬í¬ì¸íŠ¸ì—ëŠ” í•´ë‹¹ ì‹œì ì˜ ìƒíƒœ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ìƒíƒœ ì´ë ¥ì„ ì¡°íšŒí•˜ê³  íŠ¹ì • ì¡°ê±´(ë©”ì‹œì§€ ìˆ˜ê°€ 6ê°œ)ì— í•´ë‹¹í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ìƒíƒœ íˆìŠ¤í† ë¦¬ í™•ì¸\n",
    "print(\"ìƒíƒœ íˆìŠ¤í† ë¦¬ (ìµœì‹ ìˆœ):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# to_replay ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "to_replay = None\n",
    "\n",
    "for i, state in enumerate(time_travel_graph.get_state_history(time_travel_config)):\n",
    "    print(f\"\\n[ì²´í¬í¬ì¸íŠ¸ {i}]\")\n",
    "    print(f\"  ë‹¤ìŒ ë…¸ë“œ: {state.next}\")\n",
    "    print(f\"  ì²´í¬í¬ì¸íŠ¸ ID: {state.config['configurable']['checkpoint_id']}\")\n",
    "\n",
    "    if len(state.values[\"messages\"]) == 6 and to_replay is None:\n",
    "        print(\"  ì´ ìƒíƒœë¡œ ë˜ëŒì•„ê°ˆ ì˜ˆì •\")\n",
    "        display_message_tree(state.values[\"messages\"][-1])\n",
    "        to_replay = state\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ì²´í¬í¬ì¸íŠ¸ë¡œ ë¡¤ë°±\n\nì„ íƒí•œ ì²´í¬í¬ì¸íŠ¸ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì´ ì‹œì ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ë‚´ìš©ì„ í‘œì‹œí•˜ì—¬ ì–´ëŠ ì§€ì ìœ¼ë¡œ ëŒì•„ê°ˆì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ì„ íƒí•œ ì²´í¬í¬ì¸íŠ¸ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_message_tree(to_replay.values[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ìƒíƒœ ìˆ˜ì •\n\në³µì›ëœ ìƒíƒœì—ì„œ ë„êµ¬ í˜¸ì¶œ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤. `update_tool_call` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ì¡´ ë„êµ¬ í˜¸ì¶œì˜ ì¸ìë¥¼ ë³€ê²½í•  ìˆ˜ ìˆì–´, ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì¬ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” Tavily ê²€ìƒ‰ ë„êµ¬ì˜ ì¿¼ë¦¬ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import update_tool_call\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "updated_message = update_tool_call(\n",
    "    to_replay.values[\"messages\"][-1],\n",
    "    tool_name=\"tavily_search\",\n",
    "    tool_args={\"query\": \"í…Œë””ë…¸íŠ¸ ì˜¨ë¼ì¸ ê°•ì˜ site:naver.com\", \"search_depth\": \"basic\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ê²½í•˜ê¸° ì „ì˜ message\n",
    "display_message_tree(to_replay.values[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ê²½í•œ ì´í›„ì˜ ë©”ì‹œì§€ íŠ¸ë¦¬\n",
    "display_message_tree(updated_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ê²½ëœ ë©”ì‹œì§€ë¥¼ update_state ë¡œ ì—…ë°ì´íŠ¸\n",
    "updated_state = time_travel_graph.update_state(\n",
    "    values={\"messages\": [updated_message]}, config=to_replay.config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ìˆ˜ì • ìƒíƒœ ì¬ì‹¤í–‰\n\nì—…ë°ì´íŠ¸ëœ ìƒíƒœë¡œ ê·¸ë˜í”„ë¥¼ ì¬ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. `update_state` ë©”ì„œë“œë¡œ ìƒíƒœë¥¼ ìˆ˜ì •í•˜ë©´ í•´ë‹¹ ì‹œì ë¶€í„° ìƒˆë¡œìš´ ê²½ë¡œë¡œ ì‹¤í–‰ì´ ê³„ì†ë©ë‹ˆë‹¤.\n\nì•„ë˜ ì½”ë“œëŠ” ìˆ˜ì •ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ê·¸ë˜í”„ë¥¼ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—…ë°ì´íŠ¸ëœ ë©”ì‹œì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í•©ë‹ˆë‹¤.\n",
    "stream_graph(time_travel_graph, inputs=None, config=updated_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}