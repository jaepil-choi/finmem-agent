# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.19.0
#   kernelspec:
#     display_name: .venv
#     language: python
#     name: python3
# ---

# %% [markdown]
# # LangGraph V1.0 Quickstart
#
# ì´ íŠœí† ë¦¬ì–¼ì€ LangGraph V1.0ê³¼ Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì„¤ì •ë¶€í„° ì™„ì „íˆ ì‘ë™í•˜ëŠ” AI ì—ì´ì „íŠ¸ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ ì•ˆë‚´í•©ë‹ˆë‹¤. OllamaëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ LLMì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¡œ, í´ë¼ìš°ë“œ API ì—†ì´ë„ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# > ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Graph API](https://docs.langchain.com/oss/python/langgraph/graph-api.md)

# %% [markdown]
# í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ ì›í™œí•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
# ì´ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•´ì„œëŠ” `.env` ì— í‚¤ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

# %%
from dotenv import load_dotenv
from langchain_teddynote import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)
# ì¶”ì ì„ ìœ„í•œ í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •
logging.langsmith("LangChain-V1-Tutorial")

# %% [markdown]
# ## ëª¨ë¸ ì´ë¦„ ì§€ì •
#
# ëª¨ë¸ ì´ë¦„ì„ ì§€ì •í•  ë•Œ ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
#
# ### ê¸°ë³¸ í˜•ì‹
#
# ë‹¨ìˆœíˆ ëª¨ë¸ ì´ë¦„ë§Œ ì§€ì •:
# * `'o3-mini'`
# * `'claude-sonnet-4-5'`
#
# ### í†µí•© í˜•ì‹
#
# ëª¨ë¸ ì œê³µìì™€ ëª¨ë¸ì„ í•¨ê»˜ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
#
# ```
# '{model_provider}:{model}'
# ```
#
# **ì˜ˆì‹œ:**
# * `'openai:o1'`
# * `'anthropic:claude-sonnet-4-5'`
#
# ì´ í˜•ì‹ì„ ì‚¬ìš©í•˜ë©´ í•˜ë‚˜ì˜ ì¸ìë¡œ ëª¨ë¸ ì œê³µìì™€ ëª¨ë¸ì„ ë™ì‹œì— ëª…ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# **ì£¼ìš” íŒŒë¼ë¯¸í„°**
#
# * **temperature**: ì¶œë ¥ì˜ ë¬´ì‘ìœ„ì„±ì„ ì¡°ì ˆí•˜ëŠ” ëª¨ë¸ ì˜¨ë„ ê°’
# * **max_tokens**: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
# * **timeout**: ì‘ë‹µ ëŒ€ê¸° ìµœëŒ€ ì‹œê°„ (ì´ˆ ë‹¨ìœ„)
# * **max_retries**: ìš”ì²­ ì‹¤íŒ¨ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
# * **base_url**: ì»¤ìŠ¤í…€ API endpoint URL
# * **rate_limiter**: ìš”ì²­ ì†ë„ë¥¼ ì œì–´í•˜ëŠ” BaseRateLimiter ì¸ìŠ¤í„´ìŠ¤
#
# ### ì‚¬ìš© ì˜ˆì‹œ
#
# ```python
# model_kwargs = {
#     "temperature": 0.7,
#     "max_tokens": 1000,
#     "timeout": 30
# }
# ```
#
# > **ì°¸ê³ **: ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²´ íŒŒë¼ë¯¸í„° ëª©ë¡ì€ ê° ëª¨ë¸ ì œê³µìì˜ integration referenceë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.
#
# - [ê³µì‹ë¬¸ì„œ](https://reference.langchain.com/python/langchain/models/?_gl=1*kundig*_gcl_au*MjAwMTM0Mzc1Mi4xNzYxNDEwNDky*_ga*MTI0ODcwNDIuMTc2MTgwNjA5Mg..*_ga_47WX3HKKY2*czE3NjE4MDYwNzUkbzUkZzEkdDE3NjE4MDYxMjEkajE0JGwwJGgw#langchain.chat_models.init_chat_model)

# %%
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="gpt-oss:120b-cloud",
    temperature=0,
)

# %%
from langchain_teddynote.messages import stream_response

result = llm.stream("ë°˜ê°€ì›Œ")
# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
stream_response(result)

# %% [markdown]
# ## ì—ì´ì „íŠ¸ ìƒì„±
#
# LangGraph ê¸°ë°˜ì˜ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê³¼ê±° `create_react_agent` ëŒ€ì‹  `create_agent` ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

# %%
from langchain.agents import create_agent

llm = ChatOllama(
    model="gpt-oss:120b-cloud",
    temperature=0,
)

# ëª¨ë¸ ì‹ë³„ì ë¬¸ìì—´ì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë°©ë²•
agent = create_agent(llm, tools=[])

# %% [markdown]
# ### ê·¸ë˜í”„ ì‹œê°í™”
#
# `langchain_teddynote` íŒ¨í‚¤ì§€ì˜ `visualize_graph` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë…¸ë“œì™€ ì—£ì§€ì˜ ì—°ê²° ìƒíƒœë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ìƒì„±ëœ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

# %%
from langchain_teddynote.graphs import visualize_graph

visualize_graph(agent)

# %% [markdown]
# ### ë©”ì‹œì§€ ì¶œë ¥
#
# `stream_graph` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì—ì´ì „íŠ¸ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ë…¸ë“œì—ì„œ ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

# %%
from langchain_teddynote.messages import stream_graph
from langchain_core.messages import HumanMessage

stream_graph(agent, inputs={"messages": [HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”?")]})

# %% [markdown]
# ## ê¸°ë³¸ ì—ì´ì „íŠ¸ êµ¬ì¶•
#
# ì§ˆë¬¸ì— ë‹µí•˜ê³  ë„êµ¬ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” ê°„ë‹¨í•œ ì—ì´ì „íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤. 
#
# ê¸°ë³¸ ë‚ ì”¨ í•¨ìˆ˜(ì‹¤ì œë¡œ ê¸°ëŠ¥ì´ ìˆëŠ” ë„êµ¬ëŠ” ì•„ë‹™ë‹ˆë‹¤!) ë¥¼ ë„êµ¬ë¡œ ì‚¬ìš©í•˜ë©°, ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë™ì‘ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

# %%
from langchain.tools import tool
from langchain.agents import create_agent


# ë‚ ì”¨ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜
@tool
def get_weather(city: str) -> str:
    """ì£¼ì–´ì§„ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return f"It's always sunny in {city}!"


# ì—ì´ì „íŠ¸ ìƒì„±
agent = create_agent(
    model=llm,
    tools=[get_weather],
    system_prompt="You are a helpful assistant",
)

# %% [markdown]
# ### ê·¸ë˜í”„ ì‹œê°í™”
#
# ë„êµ¬ê°€ ì—°ê²°ëœ ì—ì´ì „íŠ¸ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. `model` ë…¸ë“œì™€ `tools` ë…¸ë“œ ê°„ì˜ ì—°ê²° ê´€ê³„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ë„êµ¬ê°€ ì—°ê²°ëœ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

# %%
from langchain_teddynote.graphs import visualize_graph

visualize_graph(agent)

# %% [markdown]
# ### ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ì¶œë ¥
#
# ë„êµ¬ê°€ ì—°ê²°ëœ ì—ì´ì „íŠ¸ì— ì§ˆë¬¸ì„ ë³´ë‚´ê³  ì‘ë‹µì„ í™•ì¸í•©ë‹ˆë‹¤. LLMì´ ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í•˜ë©´ ìë™ìœ¼ë¡œ `tools` ë…¸ë“œë¥¼ ê±°ì³ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ë‚ ì”¨ ì§ˆë¬¸ì„ ì—ì´ì „íŠ¸ì— ì „ë‹¬í•˜ê³  ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤.

# %%
# ì—ì´ì „íŠ¸ ì‹¤í–‰
stream_graph(agent, inputs={"messages": [HumanMessage(content="ì„œìš¸ ë‚ ì”¨ê°€ ì–´ë•Œ?")]})

# %% [markdown]
# ## ë„êµ¬(Tool)
#
# ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ì •ì˜í•œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
#
# ë„êµ¬ëŠ” ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸ì— ì˜ì¡´í•  ìˆ˜ ìˆìœ¼ë©° ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ì™€ ìƒí˜¸ì‘ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
#
# ### ì»¨íƒìŠ¤íŠ¸(Context)
#
# ì»¨íƒìŠ¤íŠ¸ëŠ” ë„êµ¬ì— ì „ë‹¬ë˜ëŠ” ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. 
#
# `runtime.context` ë¥¼ í†µí•´ ì»¨íƒìŠ¤íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ```python
# runtime.context.user_id
# ```

# %%
from dataclasses import dataclass
from langchain_ollama import ChatOllama
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime


USER_DATABASE = {
    "teddy": {
        "name": "Teddy Lee",
        "account_type": "Premium",
        "balance": 5000,
        "email": "teddy@example.com",
    },
    "shirley": {
        "name": "Shirley Kim",
        "account_type": "Standard",
        "balance": 1200,
        "email": "shirley@example.com",
    },
}


@dataclass
class UserContext:
    user_id: str


@tool
def get_account_info(runtime: ToolRuntime[UserContext]) -> str:
    """í˜„ì¬ ì‚¬ìš©ìì˜ ê³„ì¢Œ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    user_id = runtime.context.user_id

    if user_id in USER_DATABASE:
        user = USER_DATABASE[user_id]
        return f"Account holder: {user['name']}\nType: {user['account_type']}\nBalance: ${user['balance']}"
    return "User not found"

model = ChatOllama(model="gpt-oss:120b-cloud")

agent = create_agent(
    model,
    tools=[get_account_info],
    context_schema=UserContext,
    system_prompt="You are a financial assistant.",
)

# %%
from langchain_teddynote.messages import stream_graph
from langchain_core.messages import HumanMessage

stream_graph(
    agent,
    inputs={"messages": [HumanMessage(content="ë‚´ ê³„ì¢Œì˜ í˜„ì¬ ì”ê³ ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.")]},
    context=UserContext(user_id="teddy"),
)

# %% [markdown]
# ## ì‘ë‹µ í˜•ì‹(Response Format)
#
# ì—ì´ì „íŠ¸ ì‘ë‹µì´ íŠ¹ì • ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ë„ë¡ êµ¬ì¡°í™”ëœ ì‘ë‹µ í˜•ì‹ì„ ì •ì˜í•©ë‹ˆë‹¤.
#
# ì°¸ê³ : dataclass ë˜ëŠ” pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ í˜•ì‹ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# %%
from pydantic import BaseModel, Field


class ResponseFormat(BaseModel):
    """ì—ì´ì „íŠ¸ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""

    email_sender: str = Field(description="ì´ë©”ì¼ ë°œì‹ ì")
    email_sender_address: str = Field(description="ë°œì‹ ì ì£¼ì†Œ")


# %%
# ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ í¬í•¨í•œ ì—ì´ì „íŠ¸ ìƒì„±
agent = create_agent(
    model=llm,
    system_prompt="Extract useful information from the email.",
    tools=[],
    response_format=ResponseFormat,
)

sample_input = """From: ê¹€ì² ìˆ˜ (chulsoo.kim@bikecorporation.me)
Subject: "ZENESIS" ìì „ê±° ìœ í†µ í˜‘ë ¥ ë° ë¯¸íŒ… ì¼ì • ì œì•ˆ

ì•ˆë…•í•˜ì„¸ìš”, ì´ì€ì±„ ëŒ€ë¦¬ë‹˜,

ì €ëŠ” ë°”ì´í¬ì½”í¼ë ˆì´ì…˜ì˜ ê¹€ì² ìˆ˜ ìƒë¬´ì…ë‹ˆë‹¤. ìµœê·¼ ë³´ë„ìë£Œë¥¼ í†µí•´ ê·€ì‚¬ì˜ ì‹ ê·œ ìì „ê±° "ZENESIS"ì— ëŒ€í•´ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë°”ì´í¬ì½”í¼ë ˆì´ì…˜ì€ ìì „ê±° ì œì¡° ë° ìœ í†µ ë¶„ì•¼ì—ì„œ í˜ì‹ ê³¼ í’ˆì§ˆì„ ì„ ë„í•˜ëŠ” ê¸°ì—…ìœ¼ë¡œ, ì´ ë¶„ì•¼ì—ì„œì˜ ì¥ê¸°ì ì¸ ê²½í—˜ê³¼ ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ZENESIS ëª¨ë¸ì— ëŒ€í•œ ìƒì„¸í•œ ë¸Œë¡œìŠˆì–´ë¥¼ ìš”ì²­ë“œë¦½ë‹ˆë‹¤. íŠ¹íˆ ê¸°ìˆ  ì‚¬ì–‘, ë°°í„°ë¦¬ ì„±ëŠ¥, ê·¸ë¦¬ê³  ë””ìì¸ ì¸¡ë©´ì— ëŒ€í•œ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì €í¬ê°€ ì œì•ˆí•  ìœ í†µ ì „ëµê³¼ ë§ˆì¼€íŒ… ê³„íšì„ ë³´ë‹¤ êµ¬ì²´í™”í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

ë˜í•œ, í˜‘ë ¥ ê°€ëŠ¥ì„±ì„ ë” ê¹Šì´ ë…¼ì˜í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì£¼ í™”ìš”ì¼(1ì›” 15ì¼) ì˜¤ì „ 10ì‹œì— ë¯¸íŒ…ì„ ì œì•ˆí•©ë‹ˆë‹¤. ê·€ì‚¬ ì‚¬ë¬´ì‹¤ì—ì„œ ë§Œë‚˜ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆì„ê¹Œìš”?

ê°ì‚¬í•©ë‹ˆë‹¤.

ê¹€ì² ìˆ˜
ìƒë¬´ì´ì‚¬
ë°”ì´í¬ì½”í¼ë ˆì´ì…˜
"""

# ì²« ë²ˆì§¸ ì§ˆë¬¸: ë‚ ì”¨ ë¬¸ì˜
response = agent.invoke(
    {"messages": [HumanMessage(content=sample_input)]},
)
print(response["messages"][-1].content)
print("===" * 10)
print(response["structured_response"])

# %% [markdown]
# ## ë‹¨ê¸° ë©”ëª¨ë¦¬ ì¶”ê°€
#
# ì—ì´ì „íŠ¸ì— ë©”ëª¨ë¦¬ë¥¼ ì¶”ê°€í•˜ì—¬ ìƒí˜¸ì‘ìš© ê°„ì— ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ì´ì „ ëŒ€í™”ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ë‹¨ê¸° ê¸°ì–µì˜ ìœ ì§€ì˜ ë²”ìœ„ëŠ” `thread_id` ë¡œ ê´€ë¦¬ í•©ë‹ˆë‹¤. ì¦‰, ë™ì¼í•œ `thread_id` ëŠ” ë™ì¼í•œ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.
#
# ì°¸ê³ : í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì˜êµ¬ ì²´í¬í¬ì¸í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

# %%
from langgraph.checkpoint.memory import InMemorySaver

# ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ìƒì„±
checkpointer = InMemorySaver()

# %%
# ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ í¬í•¨í•œ ì—ì´ì „íŠ¸ ìƒì„±
agent = create_agent(
    model=llm,
    checkpointer=checkpointer,
)

# thread_idëŠ” íŠ¹ì • ëŒ€í™”ì˜ ê³ ìœ  ì‹ë³„ìì…ë‹ˆë‹¤.
config = {"configurable": {"thread_id": "1"}}


stream_graph(
    agent,
    inputs={"messages": [HumanMessage(content="ì•ˆë…•, ë‚´ ì´ë¦„ì€ í…Œë””ì•¼")]},
    config=config,
)

# %%
stream_graph(
    agent,
    inputs={"messages": [HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­”ì§€ ê¸°ì–µë‚˜?")]},
    config=config,
)

# %%
stream_graph(
    agent,
    inputs={"messages": [HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­”ì§€ ê¸°ì–µë‚˜?")]},
    config={"configurable": {"thread_id": "2"}},
)

# %% [markdown]
# ## ë¯¸ë“¤ì›¨ì–´(Middleware)
#
# ë¯¸ë“¤ì›¨ì–´ëŠ” ì—ì´ì „íŠ¸ ì‹¤í–‰ì˜ ëª¨ë“  ë‹¨ê³„ë¥¼ ì œì–´í•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.
#
# í•µì‹¬ ì—ì´ì „íŠ¸ ë£¨í”„ëŠ” ëª¨ë¸ì„ í˜¸ì¶œí•˜ê³ , ëª¨ë¸ì´ ì‹¤í–‰í•  ë„êµ¬ë¥¼ ì„ íƒí•˜ë„ë¡ í•œ ë‹¤ìŒ, ë” ì´ìƒ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë©´ ì¢…ë£Œí•˜ëŠ” ê²ƒì„ í¬í•¨í•©ë‹ˆë‹¤.
#
# ![](./assets/langgraph-middleware.avif)
#
# ë¯¸ë“¤ì›¨ì–´ëŠ” ê° ë‹¨ê³„ ì „í›„ì— í›„í¬ë¥¼ ë…¸ì¶œí•©ë‹ˆë‹¤.
#
# - ì—ì´ì „íŠ¸ ì‹œì‘ ì „/í›„
# - ëª¨ë¸ í˜¸ì¶œ ì „/í›„
# - ë„êµ¬ ì‹¤í–‰ ì „/í›„

# %% [markdown]
# ## Human in the Loop Middleware
#
# ### ê°œìš”
#
# Human in the Loop MiddlewareëŠ” AI ì‹œìŠ¤í…œì˜ ì˜ì‚¬ê²°ì • ê³¼ì •ì— ì‚¬ëŒì˜ ê°œì…ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì¤‘ê°„ ê³„ì¸µì…ë‹ˆë‹¤. ìë™í™”ëœ í”„ë¡œì„¸ìŠ¤ ì¤‘ íŠ¹ì • ì‹œì ì—ì„œ ì‚¬ëŒì˜ ê²€í† , ìŠ¹ì¸ ë˜ëŠ” ìˆ˜ì •ì„ ìš”êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ### ì£¼ìš” íŠ¹ì§•
#
# * **ê²€ì¦ ë‹¨ê³„ ì¶”ê°€**: AIì˜ ì¶œë ¥ì„ ì‚¬ëŒì´ ê²€í† í•˜ê³  ìŠ¹ì¸í•˜ëŠ” ë‹¨ê³„ ì‚½ì…
# * **ì˜¤ë¥˜ ë°©ì§€**: ì¤‘ìš”í•œ ê²°ì •ì— ëŒ€í•œ ì‚¬ëŒì˜ ìµœì¢… í™•ì¸ìœ¼ë¡œ ì˜¤ë¥˜ ìµœì†Œí™”
# * **ìœ ì—°í•œ ê°œì…**: í•„ìš”ì— ë”°ë¼ ìë™/ìˆ˜ë™ ëª¨ë“œ ì „í™˜ ê°€ëŠ¥
# * **í”¼ë“œë°± ë£¨í”„**: ì‚¬ëŒì˜ ìˆ˜ì • ì‚¬í•­ì„ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©
#
# ### Parameters
#
# **`timeout`**
# * **íƒ€ì…**: `int` ë˜ëŠ” `float`
# * **ê¸°ë³¸ê°’**: `None`
# * **ì„¤ëª…**: ì‚¬ëŒì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ìµœëŒ€ ì‹œê°„(ì´ˆ)
# * **ì‚¬ìš©ë²•**: timeout ì´ˆê³¼ ì‹œ ê¸°ë³¸ ë™ì‘ ì‹¤í–‰ ë˜ëŠ” ì˜ˆì™¸ ë°œìƒ
# ```python
# middleware = HumanInTheLoopMiddleware(timeout=300)  # 5ë¶„
# ```
#
# **`approval_required`**
# * **íƒ€ì…**: `bool`
# * **ê¸°ë³¸ê°’**: `True`
# * **ì„¤ëª…**: ì‚¬ëŒì˜ ëª…ì‹œì  ìŠ¹ì¸ì´ í•„ìš”í•œì§€ ì—¬ë¶€
# * **ì‚¬ìš©ë²•**: `False`ë¡œ ì„¤ì • ì‹œ ê²€í† ë§Œ í•˜ê³  ìë™ ì§„í–‰
# ```python
# middleware = HumanInTheLoopMiddleware(approval_required=True)
# ```
#
# **`callback_function`**
# * **íƒ€ì…**: `callable`
# * **ê¸°ë³¸ê°’**: `None`
# * **ì„¤ëª…**: ì‚¬ëŒì˜ ê°œì…ì´ í•„ìš”í•  ë•Œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜
# * **ì‚¬ìš©ë²•**: ì•Œë¦¼, ë¡œê¹…, UI í‘œì‹œ ë“±ì˜ ì»¤ìŠ¤í…€ ë™ì‘ ì •ì˜
# ```python
# def notify_user(data):
#     print(f"Review needed: {data}")
#
# middleware = HumanInTheLoopMiddleware(callback_function=notify_user)
# ```
#
# **`intervention_condition`**
# * **íƒ€ì…**: `callable` ë˜ëŠ” `str`
# * **ê¸°ë³¸ê°’**: `"always"`
# * **ì„¤ëª…**: ì‚¬ëŒ ê°œì…ì´ í•„ìš”í•œ ì¡°ê±´ ì •ì˜
# * **ì‚¬ìš©ë²•**: í•¨ìˆ˜ ë˜ëŠ” ì¡°ê±´ ë¬¸ìì—´ë¡œ ì§€ì •
# ```python
# # í•¨ìˆ˜ë¡œ ì¡°ê±´ ì •ì˜
# def check_confidence(result):
#     return result.confidence < 0.8
#
# middleware = HumanInTheLoopMiddleware(intervention_condition=check_confidence)
#
# # ë¬¸ìì—´ë¡œ ì¡°ê±´ ì •ì˜
# middleware = HumanInTheLoopMiddleware(intervention_condition="low_confidence")
# ```
#
# **`retry_limit`**
# * **íƒ€ì…**: `int`
# * **ê¸°ë³¸ê°’**: `3`
# * **ì„¤ëª…**: ì‚¬ëŒì˜ ì‘ë‹µì„ ìš”ì²­í•˜ëŠ” ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
# * **ì‚¬ìš©ë²•**: ì‘ë‹µì´ ì—†ì„ ë•Œ ì¬ì‹œë„ íšŸìˆ˜ ì œí•œ
# ```python
# middleware = HumanInTheLoopMiddleware(retry_limit=5)
# ```
#
# **`fallback_action`**
# * **íƒ€ì…**: `str` ë˜ëŠ” `callable`
# * **ê¸°ë³¸ê°’**: `"reject"`
# * **ì„¤ëª…**: timeout ë˜ëŠ” ì‘ë‹µ ì‹¤íŒ¨ ì‹œ ìˆ˜í–‰í•  ë™ì‘
# * **ì˜µì…˜**: `"approve"`, `"reject"`, `"skip"`, ë˜ëŠ” ì»¤ìŠ¤í…€ í•¨ìˆ˜
# * **ì‚¬ìš©ë²•**:
# ```python
# middleware = HumanInTheLoopMiddleware(fallback_action="approve")
#
# # ì»¤ìŠ¤í…€ fallback
# def custom_fallback(context):
#     return context.get("default_value")
#
# middleware = HumanInTheLoopMiddleware(fallback_action=custom_fallback)
# ```
#
# **`notification_channels`**
# * **íƒ€ì…**: `list`
# * **ê¸°ë³¸ê°’**: `["console"]`
# * **ì„¤ëª…**: ì•Œë¦¼ì„ ì „ì†¡í•  ì±„ë„ ëª©ë¡
# * **ì˜µì…˜**: `"console"`, `"email"`, `"slack"`, `"webhook"` ë“±
# * **ì‚¬ìš©ë²•**:
# ```python
# middleware = HumanInTheLoopMiddleware(
#     notification_channels=["email", "slack"]
# )
# ```
#
# **`store_feedback`**
# * **íƒ€ì…**: `bool`
# * **ê¸°ë³¸ê°’**: `True`
# * **ì„¤ëª…**: ì‚¬ëŒì˜ í”¼ë“œë°±ì„ ì €ì¥í• ì§€ ì—¬ë¶€
# * **ì‚¬ìš©ë²•**: í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©í•˜ê¸° ìœ„í•´ í”¼ë“œë°± ì €ì¥
# ```python
# middleware = HumanInTheLoopMiddleware(store_feedback=True)
# ```
#
# **`priority_level`**
# * **íƒ€ì…**: `str` ë˜ëŠ” `int`
# * **ê¸°ë³¸ê°’**: `"normal"`
# * **ì„¤ëª…**: ê°œì… ìš”ì²­ì˜ ìš°ì„ ìˆœìœ„
# * **ì˜µì…˜**: `"low"`, `"normal"`, `"high"`, `"critical"` ë˜ëŠ” 1-5
# * **ì‚¬ìš©ë²•**:
# ```python
# middleware = HumanInTheLoopMiddleware(priority_level="high")
# ```

# %%
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command
from langchain.tools import tool


@tool
def search_tool(query: str) -> str:
    """ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    return f"Search results for: {query}"


@tool
def send_email_tool(recipient: str, subject: str, body: str) -> str:
    """ì´ë©”ì¼ì„ ì „ì†¡í•©ë‹ˆë‹¤. ë¯¼ê°í•œ ì‘ì—…ì…ë‹ˆë‹¤."""
    return f"Email sent to {recipient}"


@tool
def delete_database_tool(database_name: str) -> str:
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ì‘ì—…ì…ë‹ˆë‹¤."""
    return f"Database {database_name} deleted"


agent = create_agent(
    model=llm,
    tools=[search_tool, send_email_tool, delete_database_tool],
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                # ë¯¼ê°í•œ ì‘ì—…ì— ëŒ€í•´ ìŠ¹ì¸ í•„ìš”
                "send_email_tool": True,
                "delete_database_tool": True,
                # ì•ˆì „í•œ ì‘ì—…ì€ ìë™ ìŠ¹ì¸
                "search_tool": False,
            }
        ),
    ],
    checkpointer=InMemorySaver(),  # ìƒíƒœ ì§€ì†ì„± í•„ìš”
)

# thread_id í•„ìš”
config = {"configurable": {"thread_id": "123"}}

# %%
from langchain_teddynote.messages import invoke_graph

invoke_graph(
    agent,
    inputs={
        "messages": [
            HumanMessage(
                content="teddy@example.com ì—ê²Œ ë©”ì¼ì„ ë³´ë‚´ ì£¼ì„¸ìš”. ì œëª©ì€ 'í…ŒìŠ¤íŠ¸' ì´ê³  ë‚´ìš©ì€ 'ì•ˆë…•í•˜ì„¸ìš”' ì…ë‹ˆë‹¤."
            )
        ]
    },
    config=config,
)

# %%
# interrupt í™•ì¸
print(agent.get_state(config).interrupts[0].value["action_requests"][0]["description"])

# %%
# decisions: approve, reject, skip
stream_graph(
    agent, inputs=Command(resume={"decisions": [{"type": "approve"}]}), config=config
)
