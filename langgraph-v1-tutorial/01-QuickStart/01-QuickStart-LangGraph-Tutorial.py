# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.19.0
#   kernelspec:
#     display_name: .venv
#     language: python
#     name: python3
# ---

# %% [markdown]
# # LangGraph QuickStart
#
# LangGraphëŠ” ìƒíƒœ ê¸°ë°˜ ë©€í‹° ì•¡í„° ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
#
# ## êµ¬í˜„ ê¸°ëŠ¥
#
# - ìƒíƒœ ê´€ë¦¬ ê¸°ë°˜ ì±—ë´‡
# - ì™¸ë¶€ ë„êµ¬ ì—°ë™ (Tavily Search)
# - ë©”ëª¨ë¦¬ ë° ì²´í¬í¬ì¸íŠ¸
# - Human-in-the-Loop
# - ìƒíƒœ ì»¤ìŠ¤í„°ë§ˆì´ì§•
# - ìƒíƒœ ì´ë ¥ ê´€ë¦¬

# %% [markdown]
# ## í™˜ê²½ ì„¤ì •
#
# LangGraphë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ì™€ ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤. `.env` íŒŒì¼ì— API í‚¤ë¥¼ ì €ì¥í•˜ê³ , LangSmithë¥¼ í†µí•´ ì‹¤í–‰ ê³¼ì •ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê³  LangSmith í”„ë¡œì íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

# %%
from dotenv import load_dotenv
from langchain_teddynote import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(override=True)
# ì¶”ì ì„ ìœ„í•œ í”„ë¡œì íŠ¸ ì´ë¦„ ì„¤ì •
logging.langsmith("LangChain-V1-Tutorial")

# %% [markdown]
# ---
#
# ## ê¸°ë³¸ ì±—ë´‡ êµ¬ì¶•
#
# ë©”ì‹œì§€ ê¸°ë°˜ ì±—ë´‡ì„ StateGraphë¡œ êµ¬ì„±í•©ë‹ˆë‹¤. StateGraphëŠ” LangGraphì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¡œ, ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì—°ê²°í•˜ì—¬ ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
#
# > ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Graph API](https://docs.langchain.com/oss/python/langgraph/graph-api.md)
#
# ### êµ¬ì„± ìš”ì†Œ
#
# | êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
# |----------|------|
# | StateGraph | ì „ì²´ ì›Œí¬í”Œë¡œìš° íë¦„ì„ ì •ì˜í•˜ëŠ” ê·¸ë˜í”„ |
# | State | ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ìƒíƒœ ê°ì²´ |
# | Node | ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ì˜ˆ: LLM í˜¸ì¶œ) |
# | Edge | ë…¸ë“œ ê°„ ì‹¤í–‰ ê²½ë¡œë¥¼ ì—°ê²° |
# | Compile/Invoke | ê·¸ë˜í”„ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜ ë° í˜¸ì¶œ |
#
# ì•„ë˜ ì½”ë“œëŠ” State íƒ€ì…ì„ ì •ì˜í•˜ê³  StateGraph ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

# %%
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages


# State ì •ì˜: ì±—ë´‡ì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” íƒ€ì…
class State(TypedDict):
    """ì±—ë´‡ì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” íƒ€ì…

    messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    - add_messages í•¨ìˆ˜ë¥¼ í†µí•´ ìƒˆ ë©”ì‹œì§€ê°€ ì¶”ê°€ë¨ (ë®ì–´ì“°ê¸°ê°€ ì•„ë‹Œ ì¶”ê°€)
    """

    messages: Annotated[list, add_messages]


# StateGraph ìƒì„±
graph_builder = StateGraph(State)

print("StateGraph ìƒì„± ì™„ë£Œ!")
print("StateëŠ” messages í‚¤ë¥¼ ê°€ì§€ë©°, add_messages ë¦¬ë“€ì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# %% [markdown]
# ### LLM ì„¤ì •
#
# LangGraphì˜ ë…¸ë“œì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤. `init_chat_model` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ì–‘í•œ ì œê³µì(OpenAI, Anthropic ë“±)ì˜ ëª¨ë¸ì„ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” Anthropicì˜ Claude Sonnet 4.5 ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

# %%
from langchain.chat_models import init_chat_model

# ëª¨ë¸ ì‹ë³„ì ë¬¸ìì—´ì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë°©ë²•
llm = init_chat_model("anthropic:claude-sonnet-4-5")


# %% [markdown]
# ### ì±—ë´‡ ë…¸ë“œ ì¶”ê°€
#
# ë…¸ë“œëŠ” ê·¸ë˜í”„ì—ì„œ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì±—ë´‡ ë…¸ë“œëŠ” í˜„ì¬ ìƒíƒœì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ LLMì— ì „ë‹¬í•˜ê³ , ì‘ë‹µì„ ìƒˆ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
#
# `add_node` ë©”ì„œë“œì˜ ì²« ë²ˆì§¸ ì¸ìëŠ” ë…¸ë“œì˜ ê³ ìœ  ì´ë¦„ì´ê³ , ë‘ ë²ˆì§¸ ì¸ìëŠ” í˜¸ì¶œë  í•¨ìˆ˜ì…ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì±—ë´‡ ë…¸ë“œ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ê·¸ë˜í”„ì— ì¶”ê°€í•©ë‹ˆë‹¤.

# %%
def chatbot(state: State):
    """ì±—ë´‡ ë…¸ë“œ í•¨ìˆ˜

    í˜„ì¬ ìƒíƒœì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ LLMì— ì „ë‹¬í•˜ê³ ,
    ì‘ë‹µì„ ìƒˆ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±
    response = llm.invoke(state["messages"])

    # ì‘ë‹µì„ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì—¬ ë°˜í™˜
    return {"messages": [response]}


# ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€
# ì²« ë²ˆì§¸ ì¸ì: ë…¸ë“œì˜ ê³ ìœ  ì´ë¦„
# ë‘ ë²ˆì§¸ ì¸ì: ë…¸ë“œê°€ ì‚¬ìš©ë  ë•Œ í˜¸ì¶œë  í•¨ìˆ˜
graph_builder.add_node("chatbot", chatbot)

# %% [markdown]
# ### ì—£ì§€ ì„¤ì •
#
# ì—£ì§€ëŠ” ë…¸ë“œ ê°„ì˜ ì‹¤í–‰ íë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤. `START`ëŠ” ê·¸ë˜í”„ ì‹¤í–‰ì˜ ì‹œì‘ì ì´ê³ , `END`ëŠ” ì¢…ë£Œì ì…ë‹ˆë‹¤. ëª¨ë“  ê·¸ë˜í”„ëŠ” ë°˜ë“œì‹œ ì‹œì‘ì ê³¼ ì¢…ë£Œì ì„ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì‹¤í–‰ ê²½ë¡œ(START â†’ chatbot â†’ END)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

# %%
# ì§„ì…ì : ê·¸ë˜í”„ ì‹¤í–‰ì´ ì‹œì‘ë˜ëŠ” ì§€ì 
graph_builder.add_edge(START, "chatbot")

# ì¢…ë£Œì : ê·¸ë˜í”„ ì‹¤í–‰ì´ ëë‚˜ëŠ” ì§€ì 
graph_builder.add_edge("chatbot", END)

print("ì§„ì…ì ê³¼ ì¢…ë£Œì  ì„¤ì • ì™„ë£Œ!")
print("ì‹¤í–‰ íë¦„: START â†’ chatbot â†’ END")

# %% [markdown]
# ### ê·¸ë˜í”„ ì»´íŒŒì¼
#
# StateGraphë¥¼ ì •ì˜í•œ í›„ì—ëŠ” ë°˜ë“œì‹œ `compile()` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì»´íŒŒì¼ ê³¼ì •ì—ì„œ ë…¸ë“œ ê°„ ì—°ê²°ì´ ê²€ì¦ë˜ê³ , ì‹¤í–‰ ìˆœì„œê°€ ê²°ì •ë©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì •ì˜í•œ ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

# %%
# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = graph_builder.compile()

print("ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ!")

# %% [markdown]
# ### ê·¸ë˜í”„ ì‹œê°í™”
#
# ì»´íŒŒì¼ëœ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `langchain_teddynote` íŒ¨í‚¤ì§€ì˜ `visualize_graph` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ë…¸ë“œì™€ ì—£ì§€ì˜ ì—°ê²° ìƒíƒœë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì»´íŒŒì¼ëœ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

# %%
from langchain_teddynote.graphs import visualize_graph

# ê·¸ë˜í”„ ì‹œê°í™”
visualize_graph(graph)

# %% [markdown]
# ### ê·¸ë˜í”„ ì‹¤í–‰
#
# `stream_graph` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ê·¸ë˜í”„ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `RunnableConfig`ë¥¼ í†µí•´ ì¬ê·€ ê¹Šì´ ì œí•œ(`recursion_limit`)ê³¼ ìŠ¤ë ˆë“œ ì‹ë³„ì(`thread_id`)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” Configë¥¼ ì„¤ì •í•˜ê³  ì‚¬ìš©ì ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

# %%
from langchain_teddynote.messages import stream_graph
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import HumanMessage

# Config ì„¤ì •(recursion_limit: ì¬ê·€ ê¹Šì´ ì œí•œ, thread_id: ìŠ¤ë ˆë“œ ì•„ì´ë””)
config = RunnableConfig(recursion_limit=20, thread_id="abc123")

# %%
inputs = {
    "messages": [HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”! LangGraphì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.")]
}

# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¬ë°
stream_graph(graph, inputs=inputs, config=config)

# %% [markdown]
# ---
#
# ## ë„êµ¬(Tools) ì¶”ê°€
#
# ì™¸ë¶€ ê²€ìƒ‰ ë„êµ¬ë¥¼ í†µí•©í•˜ì—¬ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆëŠ” ì±—ë´‡ì„ ë§Œë“­ë‹ˆë‹¤. LangGraphëŠ” LLMì´ ì™¸ë¶€ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³  ê·¸ ê²°ê³¼ë¥¼ í™œìš©í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì‰½ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# > ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Tool Integration](https://docs.langchain.com/oss/python/langgraph/tool-integration.md)
#
# ### í•µì‹¬ ê°œë…
#
# | ê°œë… | ì„¤ëª… |
# |------|------|
# | Tool Binding | LLMì— ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ì—°ê²°í•˜ëŠ” ê³¼ì • |
# | Tool Node | ì‹¤ì œ ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•˜ëŠ” ë…¸ë“œ |
# | Conditional Edges | LLM ì‘ë‹µì— ë”°ë¼ ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ ìë™ ë¶„ê¸° |
#
# ì•„ë˜ ì½”ë“œëŠ” ê°„ë‹¨í•œ ë§ì…ˆ ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

# %%
# ê³µìœ 
from langchain_core.tools import tool


@tool
def add(a: int, b: int):
    "ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤."
    return a + b


# %%
from langchain_tavily import TavilySearch
from langgraph.prebuilt import ToolNode, tools_condition

# Tavily ê²€ìƒ‰ ë„êµ¬ ì„¤ì •
tool = TavilySearch(max_results=2)
tools = [tool, add]

# ë„êµ¬ í…ŒìŠ¤íŠ¸
result = tool.invoke("LangGraphë€ ë¬´ì—‡ì¸ê°€ìš”?")
print(f"ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(result['results'])}ê°œ")
print(f"ì²« ë²ˆì§¸ ê²°ê³¼ ì œëª©: {result['results'][0]['title']}")

# %% [markdown]
# ### ë„êµ¬ ì‚¬ìš© ê·¸ë˜í”„ êµ¬ì„±
#
# ê¸°ë³¸ ì±—ë´‡ íë¦„ì— ë„êµ¬ í˜¸ì¶œ ê²½ë¡œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. LLMì´ ë„êµ¬ í˜¸ì¶œì„ ìš”ì²­í•˜ë©´ "tools" ë…¸ë“œë¡œ ì´ë™í•˜ê³ , ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì±—ë´‡ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ìˆœí™˜ êµ¬ì¡°(chatbot â‡„ tools)ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” LLMì— ë„êµ¬ë¥¼ ë°”ì¸ë”©í•©ë‹ˆë‹¤.

# %%
llm_with_tools = llm.bind_tools(tools)

# %%
ret1 = llm_with_tools.invoke("LangGraph ê°€ ë­ì•¼?")
ret2 = llm_with_tools.invoke("LangGraph ê°€ ë­ì•¼? ê²€ìƒ‰í•´ì„œ ì•Œë ¤ì¤˜")

# %%
print(ret1.content)

# %%
from langchain_teddynote.messages import display_message_tree

display_message_tree(ret1)

# %%
display_message_tree(ret2)

# %%
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages


# State ì •ì˜ (ë™ì¼)
class State(TypedDict):
    messages: Annotated[list, add_messages]


# ìƒˆë¡œìš´ ê·¸ë˜í”„ ë¹Œë” ìƒì„±
builder = StateGraph(State)

# LLMì— ë„êµ¬ ë°”ì¸ë”© - LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
llm_with_tools = llm.bind_tools(tools)


def chatbot(state: State):
    """ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì±—ë´‡ ë…¸ë“œ"""
    # ë„êµ¬ê°€ ë°”ì¸ë”©ëœ LLM í˜¸ì¶œ
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}


# ë…¸ë“œ ì¶”ê°€
builder.add_node("chatbot", chatbot)

# ToolNode ì¶”ê°€ - ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ
tool_node = ToolNode(tools=tools)
builder.add_node("tools", tool_node)

# %% [markdown]
# ### ì¡°ê±´ë¶€ ë¼ìš°íŒ…
#
# `tools_condition`ì´ ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ì˜ `tool_calls` ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•´ ê²½ë¡œë¥¼ ë¶„ê¸°í•©ë‹ˆë‹¤.

# %% [markdown]
# ### tools_condition ë™ì‘
#
# `tool_calls` ì¡´ì¬ ì‹œ "tools"ë¡œ, ì—†ìœ¼ë©´ "\_\_end\_\_"ë¡œ ë¶„ê¸°í•©ë‹ˆë‹¤.
#
# ```python
# def tools_condition(state) -> Literal["tools", "__end__"]:
#     ai_message = state[-1] if isinstance(state, list) else state["messages"][-1]
#     return "tools" if getattr(ai_message, "tool_calls", []) else "__end__"
# ```

# %%
# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
# tools_conditionì€ ë©”ì‹œì§€ì— tool_callsê°€ ìˆìœ¼ë©´ "tools"ë¡œ,
# ì—†ìœ¼ë©´ ENDë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤
builder.add_conditional_edges(
    "chatbot",
    tools_condition,  # ì‚¬ì „ ì •ì˜ëœ ì¡°ê±´ í•¨ìˆ˜ ì‚¬ìš©
)
# Literal["tools", "__end__"]

# ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì±—ë´‡ìœ¼ë¡œ ëŒì•„ê°€ê¸°
builder.add_edge("tools", "chatbot")

# ì‹œì‘ì  ì„¤ì •
builder.add_edge(START, "chatbot")

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph_with_tools = builder.compile()

# %% [markdown]
# ### ê·¸ë˜í”„ ì‹œê°í™”
#
# ë„êµ¬ê°€ ì—°ê²°ëœ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. chatbot ë…¸ë“œì—ì„œ ì¡°ê±´ì— ë”°ë¼ tools ë…¸ë“œë¡œ ë¶„ê¸°í•˜ëŠ” êµ¬ì¡°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ë„êµ¬ê°€ ì—°ê²°ëœ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

# %%
# ê·¸ë˜í”„ ì‹œê°í™”
visualize_graph(graph_with_tools)

# %% [markdown]
# ### ë„êµ¬ ì‚¬ìš© í…ŒìŠ¤íŠ¸
#
# Tavily ê²€ìƒ‰ ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. LLMì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í•˜ë©´ ìë™ìœ¼ë¡œ ê²€ìƒ‰ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” 2025ë…„ LangGraph ì‚¬ìš© ì‚¬ë¡€ì— ëŒ€í•œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

# %%
from langchain_teddynote.messages import stream_graph

stream_graph(
    graph_with_tools,
    inputs={
        "messages": [HumanMessage(content="2025ë…„ LangGraph ì‚¬ìš© ì‚¬ë¡€ ì•Œë ¤ì£¼ì„¸ìš”.")]
    },
    config=config,
)

# %% [markdown]
# ---
#
# ## ë©”ëª¨ë¦¬ ì¶”ê°€
#
# ì„¸ì…˜ ê°„ ì‚¬ìš©ì ì •ë³´ë¥¼ ìœ ì§€í•˜ëŠ” ì˜êµ¬ ìƒíƒœ ê´€ë¦¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ í†µí•´ ì±—ë´‡ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³ , ì‚¬ìš©ìë³„ë¡œ ê°œì¸í™”ëœ ì‘ë‹µì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# > ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Persistence](https://docs.langchain.com/oss/python/langgraph/persistence.md)
#
# ### í•µì‹¬ ê°œë…
#
# | ê°œë… | ì„¤ëª… |
# |------|------|
# | Checkpointer | ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ë³µì›í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ |
# | Thread ID | ë™ì¼ ì„¸ì…˜ì„ ì‹ë³„í•˜ëŠ” ê³ ìœ  ì‹ë³„ì |
# | User ID | ì‚¬ìš©ìë³„ ì¥ê¸° ê¸°ì–µì„ ê´€ë¦¬í•˜ëŠ” ì‹ë³„ì |
# | Persistent State | ëˆ„ì ëœ ëŒ€í™” ì´ë ¥ ê¸°ë°˜ì˜ ì»¨í…ìŠ¤íŠ¸ |
#
# ì•„ë˜ ì½”ë“œëŠ” ë©”ëª¨ë¦¬ ì¶”ì¶œì„ ìœ„í•œ Pydantic ëª¨ë¸ê³¼ ì¶”ì¶œê¸°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

# %%
from typing import List, Optional
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import PydanticOutputParser
import os


# Pydantic ëª¨ë¸ ì •ì˜
class MemoryItem(BaseModel):
    """ê°œë³„ ë©”ëª¨ë¦¬ ì•„ì´í…œ"""

    key: str = Field(description="ë©”ëª¨ë¦¬ í‚¤ (ì˜ˆ: user_name, preference, fact)")
    value: str = Field(description="ë©”ëª¨ë¦¬ ê°’")
    category: str = Field(
        description="ì¹´í…Œê³ ë¦¬ (personal_info, preference, interest, relationship, fact, etc.)"
    )
    importance: int = Field(description="ì¤‘ìš”ë„ (1-5, 5ê°€ ê°€ì¥ ì¤‘ìš”)", ge=1, le=5)
    confidence: float = Field(description="ì¶”ì¶œ ì‹ ë¢°ë„ (0.0-1.0)", ge=0.0, le=1.0)


class ExtractedMemories(BaseModel):
    """ì¶”ì¶œëœ ë©”ëª¨ë¦¬ ì»¬ë ‰ì…˜"""

    memories: List[MemoryItem] = Field(description="ì¶”ì¶œëœ ë©”ëª¨ë¦¬ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸")
    summary: str = Field(description="ëŒ€í™” ë‚´ìš© ìš”ì•½")
    timestamp: str = Field(
        default_factory=lambda: datetime.now().isoformat(), description="ì¶”ì¶œ ì‹œê°„"
    )


# ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
DEFAULT_SYSTEM_PROMPT = """You are an expert memory extraction assistant. Your task is to extract important information from user conversations and convert them into structured key-value pairs for long-term memory storage.

Extract ALL relevant information from the conversation, including:
- Personal information (name, age, location, occupation, etc.)
- Preferences and interests
- Relationships and social connections
- Important facts or events mentioned
- Opinions and beliefs
- Goals and aspirations
- Any other notable information

For each piece of information:
1. Create a concise, searchable key
2. Store the complete value
3. Categorize appropriately
4. Assess importance (1-5 scale)
5. Evaluate extraction confidence (0.0-1.0)"""


def create_memory_extractor(
    model_name: Optional[str] = "anthropic:claude-sonnet-4-5",
    system_prompt: Optional[str] = None,
) -> any:
    """ë©”ëª¨ë¦¬ ì¶”ì¶œê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        model: ì‚¬ìš©í•  ì–¸ì–´ ëª¨ë¸. Noneì¼ ê²½ìš° ê¸°ë³¸ ChatOpenAI ëª¨ë¸ ì‚¬ìš©
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸. Noneì¼ ê²½ìš° ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©

    Returns:
        ë©”ëª¨ë¦¬ ì¶”ì¶œ ì²´ì¸
    """
    # Output Parser ìƒì„±
    memory_parser = PydanticOutputParser(pydantic_object=ExtractedMemories)

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if system_prompt is None:
        system_prompt = DEFAULT_SYSTEM_PROMPT

    # ì „ì²´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
    template = f"""{system_prompt}

User Input: {{input}}

{{format_instructions}}

Remember to:
- Extract multiple memory items if the conversation contains various pieces of information
- Use clear, consistent key naming conventions
- Preserve context in values when necessary
- Be comprehensive but avoid redundancy
"""

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_template(
        template,
        partial_variables={
            "format_instructions": memory_parser.get_format_instructions()
        },
    )

    # ëª¨ë¸ ì„¤ì •
    model = init_chat_model(model_name)

    # ë©”ëª¨ë¦¬ ì¶”ì¶œ ì²´ì¸ ìƒì„±
    memory_extractor = prompt | model | memory_parser

    return memory_extractor


# %%
from typing import Any
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.store.base import BaseStore
from langchain_openai import ChatOpenAI

from langchain_teddynote.memory import create_memory_extractor
import uuid

model = init_chat_model("anthropic:claude-sonnet-4-5")
memory_extractor = create_memory_extractor(model="anthropic:claude-sonnet-4-5")


def call_model(
    state: MessagesState,
    config: RunnableConfig,
    *,
    store: BaseStore,
) -> dict[str, Any]:
    """LLM ëª¨ë¸ì„ í˜¸ì¶œí•˜ê³  ì‚¬ìš©ì ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.

    Args:
        state (MessagesState): ë©”ì‹œì§€ë¥¼ í¬í•¨í•˜ëŠ” í˜„ì¬ ìƒíƒœ
        config (RunnableConfig): ì‹¤í–‰ ê°€ëŠ¥ êµ¬ì„±
        store (BaseStore): ë©”ëª¨ë¦¬ ì €ì¥ì†Œ
    """
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œ user_id ì¶”ì¶œ
    user_id = config["configurable"]["user_id"]
    namespace = ("memories", user_id)

    print(namespace)

    # ìœ ì €ì˜ ë©”ëª¨ë¦¬ ê²€ìƒ‰
    memories = store.search(namespace, query=str(state["messages"][-1].content))
    info = "\n".join([f"{memory.key}: {memory.value}" for memory in memories])
    system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

    # ì‚¬ìš©ìê°€ ê¸°ì–µ ìš”ì²­ ì‹œ ë©”ëª¨ë¦¬ ì €ì¥
    last_message = state["messages"][-1]
    if "remember" in last_message.content.lower():
        result = memory_extractor.invoke({"input": str(state["messages"][-1].content)})
        for memory in result.memories:
            print(memory)
            print("-" * 100)
            store.put(namespace, str(uuid.uuid4()), {memory.key: memory.value})

    # LLM í˜¸ì¶œ
    response = model.invoke(
        [{"role": "system", "content": system_msg}] + state["messages"]
    )
    return {"messages": response}


# %%
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore

# ê·¸ë˜í”„ ë¹Œë“œ
builder = StateGraph(MessagesState)
builder.add_node("call_model", call_model)
builder.add_edge(START, "call_model")

# ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ìƒì„±
# ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” PostgresSaver ì‚¬ìš© ê¶Œì¥
memory_saver = InMemorySaver()
memory_store = InMemoryStore()

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph_with_memory = builder.compile(
    checkpointer=memory_saver,
    store=memory_store,
)

# %%
from langchain_teddynote.messages import stream_graph


def run_graph(
    msg,
    thread_id="default",
    user_id="default",
):
    config = {
        "configurable": {
            "thread_id": thread_id + user_id,
            "user_id": user_id,
        }
    }
    print(f"\n[ìœ ì €] {msg}")
    stream_graph(
        graph_with_memory,
        inputs={"messages": [{"role": "user", "content": msg}]},
        config=config,
    )
    print()


# %%
# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬
run_graph("ì•ˆë…•? ë‚´ ì´ë¦„ì€ í…Œë””ì•¼", "1", "someone")

# %%
# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬
run_graph("ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?", "1", "someone")

# %%
# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬
run_graph("ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?", "2", "someone")

# %% [markdown]
# ### ì¥ê¸° ê¸°ì–µ ì €ì¥
#
# ë©”ì‹œì§€ì— `remember` í‚¤ì›Œë“œ í¬í•¨ ì‹œ ì¥ê¸° ì €ì¥ì†Œì— ì •ë³´ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.

# %%
# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬
run_graph("ë‚´ ì´ë¦„ì´ í…Œë””ì•¼ remember", "2", "someone")

# %% [markdown]
# ### Thread ê°„ ì§€ì†ì„±
#
# User ID ê¸°ë°˜ ì¥ê¸° ê¸°ì–µì€ Threadê°€ ë‹¬ë¼ë„ ìœ ì§€ë©ë‹ˆë‹¤.

# %%
# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬
run_graph("ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆë”ë¼?", "1004", "someone")

# %%
# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬
run_graph(
    "ë‚´ ì§ì—…ì€ AI Engineer ì•¼. ë‚´ ì·¨ë¯¸ëŠ” Netflix ë³´ê¸° ì•¼. remember", "4", "someone"
)

# %%
# ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
run_graph("ë‚´ ì´ë¦„, ì§ì—…, ì·¨ë¯¸ ì•Œë ¤ì¤˜", "100", "someone")

# %%
# ë‹¤ë¥¸ user_id ë¡œ ì‹¤í–‰í•œ ê²½ìš°
run_graph("ë‚´ ì´ë¦„, ì§ì—…, ì·¨ë¯¸ ì•Œë ¤ì¤˜", "100", "other")

# %% [markdown]
# ### State í™•ì¸
#
# ì €ì¥ëœ ìƒíƒœë¥¼ ì¡°íšŒí•˜ì—¬ ë©”ì‹œì§€ ì´ë ¥ê³¼ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. `get_state` ë©”ì„œë“œë¥¼ í†µí•´ í˜„ì¬ ìƒíƒœì˜ ìŠ¤ëƒ…ìƒ·ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” íŠ¹ì • Configì— ëŒ€í•œ í˜„ì¬ ìƒíƒœ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

# %%
# ì„ì˜ì˜ Config ì„¤ì •
config = {
    "configurable": {
        "thread_id": "100" + "someone",
        "user_id": "someone",
    }
}

# í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
snapshot = graph_with_memory.get_state(config)

print("í˜„ì¬ ìƒíƒœ ì •ë³´:")
print(f"- ë©”ì‹œì§€ ìˆ˜: {len(snapshot.values['messages'])}ê°œ")
print(f"- ì²´í¬í¬ì¸íŠ¸ ID: {snapshot.config['configurable']['checkpoint_id']}")

# ìµœê·¼ ë©”ì‹œì§€ ëª‡ ê°œ í‘œì‹œ
print("\n[ìµœê·¼ ë©”ì‹œì§€]")
for msg in snapshot.values["messages"]:
    role = msg.type if hasattr(msg, "type") else "unknown"
    content = msg.content if hasattr(msg, "content") else str(msg)
    print(f"  [{role}]: {content}")

# %% [markdown]
# ---
#
# ## Human-in-the-Loop
#
# ê³ ìœ„í—˜ ì‘ì—…ì— ëŒ€í•´ ì¸ê°„ ìŠ¹ì¸ì„ ìš”ì²­í•˜ëŠ” íë¦„ì„ ë„ì…í•©ë‹ˆë‹¤. ì¤‘ìš”í•œ ê²°ì •ì´ë‚˜ ë¯¼ê°í•œ ì‘ì—… ìˆ˜í–‰ ì „ì— ì‚¬ëŒì˜ í™•ì¸ì„ ë°›ì„ ìˆ˜ ìˆì–´ AI ì‹œìŠ¤í…œì˜ ì•ˆì „ì„±ì„ ë†’ì…ë‹ˆë‹¤.
#
# > ğŸ“– **ì°¸ê³  ë¬¸ì„œ**: [LangGraph Interrupts](https://docs.langchain.com/oss/python/langgraph/interrupts.md)
#
# ### í•µì‹¬ ê°œë…
#
# | ê°œë… | ì„¤ëª… |
# |------|------|
# | interrupt | ê·¸ë˜í”„ ì‹¤í–‰ì„ ì¼ì‹œì •ì§€í•˜ê³  ì™¸ë¶€ ì…ë ¥ì„ ëŒ€ê¸° |
# | Command | ìŠ¹ì¸/ê±°ë¶€ í›„ ì¬ê°œ ëª…ë ¹ì„ ì „ë‹¬í•˜ëŠ” ê°ì²´ |
# | Human Approval | ì‚¬ëŒì´ ê²€í† í•˜ê³  ìŠ¹ì¸í•˜ëŠ” ì›Œí¬í”Œë¡œìš° |
#
# ì•„ë˜ ì½”ë“œëŠ” ì‚¬ëŒì˜ ë„ì›€ì„ ìš”ì²­í•˜ëŠ” ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

# %%
from langchain_core.tools import tool
from langgraph.types import Command, interrupt


@tool
def human_assistance(query: str) -> str:
    """Request assistance from an expert(human)."""
    # interruptë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰ ì¼ì‹œ ì¤‘ì§€
    # ì‚¬ëŒì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¼
    human_response = interrupt({"query": query})

    # ì‚¬ëŒì˜ ì‘ë‹µ ë°˜í™˜
    return human_response["data"]


# %% [markdown]
# ### HITL ê·¸ë˜í”„ êµ¬ì„±
#
# `human_assistance` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ ë„êµ¬ê°€ í˜¸ì¶œë˜ë©´ `interrupt`ë¡œ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ê³ , ì‚¬ëŒì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. ì‘ë‹µì´ ì…ë ¥ë˜ë©´ í•´ë‹¹ ë‚´ìš©ì„ ë°˜í™˜í•˜ì—¬ ëŒ€í™”ë¥¼ ê³„ì†í•©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” Human-in-the-Loop ê¸°ëŠ¥ì´ í¬í•¨ëœ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

# %%
# ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
tools_with_human = [human_assistance]

# ìƒˆë¡œìš´ ê·¸ë˜í”„ êµ¬ì„±
graph_builder_hitl = StateGraph(State)

# LLMì— ë„êµ¬ ë°”ì¸ë”©
llm_with_human_tools = llm.bind_tools(tools_with_human)


def chatbot_with_human(state: State):
    """Human Interuption ìš”ì²­í•  ìˆ˜ ìˆëŠ” ì±—ë´‡"""
    message = llm_with_human_tools.invoke(state["messages"])

    # interrupt ì¤‘ ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ ë°©ì§€
    # (ì¬ê°œ ì‹œ ë„êµ¬ í˜¸ì¶œì´ ë°˜ë³µë˜ëŠ” ê²ƒì„ ë°©ì§€)
    if hasattr(message, "tool_calls"):
        assert (
            len(message.tool_calls) <= 1
        ), "ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œì€ interruptì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

    return {"messages": [message]}


# ë…¸ë“œ ì¶”ê°€
graph_builder_hitl.add_node("chatbot_with_human", chatbot_with_human)

# ToolNode ì¶”ê°€
tool_node_hitl = ToolNode(tools=tools_with_human)
graph_builder_hitl.add_node("tools", tool_node_hitl)

# ì—£ì§€ ì¶”ê°€
graph_builder_hitl.add_conditional_edges("chatbot_with_human", tools_condition)
graph_builder_hitl.add_edge("tools", "chatbot_with_human")
graph_builder_hitl.add_edge(START, "chatbot_with_human")

# ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼
memory_hitl = InMemorySaver()
graph_hitl = graph_builder_hitl.compile(checkpointer=memory_hitl)

# ê·¸ë˜í”„ ì‹œê°í™”
visualize_graph(graph_hitl)

# %% [markdown]
# ### HITL í…ŒìŠ¤íŠ¸
#
# ì‚¬ëŒì—ê²Œ ì¡°ì–¸ì„ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ interruptì™€ ì¬ê°œ íë¦„ì„ ê²€ì¦í•©ë‹ˆë‹¤. ê·¸ë˜í”„ê°€ ì¤‘ë‹¨ë˜ë©´ `get_state`ë¡œ í˜„ì¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , `Command(resume=...)`ë¡œ ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì¸ê°„ ì§€ì›ì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€ë¡œ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

# %%
from langchain_teddynote.messages import random_uuid

# ì¸ê°„ ì§€ì›ì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€
user_input = "LangGraph ê°€ ë­ì•¼? ì‚¬ëŒí•œí…Œ ë“£ê³  ì‹¶ì–´."
config_hitl = {"configurable": {"thread_id": random_uuid()}}

print(f"User: {user_input}\n")

stream_graph(
    graph_hitl,
    inputs={"messages": [HumanMessage(content=user_input)]},
    config=config_hitl,
)

# %%
# ìƒíƒœ í™•ì¸ - ì–´ëŠ ë…¸ë“œì—ì„œ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
snapshot = graph_hitl.get_state(config_hitl)
print(f"\ní˜„ì¬ ìƒíƒœ:")
print(f"  ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ: {snapshot.next}")
print(f"  ì²´í¬í¬ì¸íŠ¸ ID: {snapshot.config['configurable']['checkpoint_id']}")

# %%
# ì¸ê°„ì˜ ì‘ë‹µìœ¼ë¡œ ì‹¤í–‰ ì¬ê°œ
human_response = """## ì „ë¬¸ê°€ì˜ ì¡°ì–¸:
- YouTube í…Œë””ë…¸íŠ¸: https://www.youtube.com/c/teddynote
- ê³ ê¸‰ ê°œë°œì ê°•ì˜ [íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ RAG ë¹„ë²•ë…¸íŠ¸](https://fastcampus.co.kr/data_online_teddy)
"""

# Command ê°ì²´ë¡œ ì¬ê°œ
human_command = Command(resume={"data": human_response})

print(f"\nì‚¬ëŒì˜ ì‘ë‹µ: {human_response}\n")

# ì¬ê°œ
stream_graph(graph_hitl, inputs=human_command, config=config_hitl)

# %% [markdown]
# ---
#
# ## ìƒíƒœ ì»¤ìŠ¤í„°ë§ˆì´ì§•
#
# ë©”ì‹œì§€ ì™¸ì— ì—…ë¬´ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì»¤ìŠ¤í…€ ìƒíƒœì™€ ë„êµ¬ ê¸°ë°˜ ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ë„ì…í•©ë‹ˆë‹¤. `Command` ê°ì²´ë¥¼ ì‚¬ìš©í•˜ë©´ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¡œ ìƒíƒœë¥¼ ì§ì ‘ ê°±ì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ### í•µì‹¬ ê°œë…
#
# | ê°œë… | ì„¤ëª… |
# |------|------|
# | Custom State Fields | messages ì™¸ì— ì¶”ê°€ í•„ë“œë¥¼ ì •ì˜ |
# | State Updates from Tools | ë„êµ¬ ê²°ê³¼ë¡œ ìƒíƒœë¥¼ ì§ì ‘ ê°±ì‹  |
# | Command(update=...) | ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ì§€ì •í•˜ëŠ” ëª…ë ¹ ê°ì²´ |
#
# ì•„ë˜ ì½”ë“œëŠ” `human_feedback` í•„ë“œê°€ ì¶”ê°€ëœ ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

# %%
from langchain_core.messages import ToolMessage
from langchain_core.tools import InjectedToolCallId


# í™•ì¥ëœ State ì •ì˜
class CustomState(TypedDict):
    """ì»¤ìŠ¤í…€ í•„ë“œê°€ ì¶”ê°€ëœ ìƒíƒœ"""

    messages: Annotated[list, add_messages]
    human_feedback: str  # ì‚¬ëŒì˜ í”¼ë“œë°±


# %% [markdown]
# ### ìƒíƒœ ì—…ë°ì´íŠ¸ ë„êµ¬
#
# ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ `Command(update=...)`ë¡œ ìƒíƒœì— ë°˜ì˜í•©ë‹ˆë‹¤. ì´ íŒ¨í„´ì„ ì‚¬ìš©í•˜ë©´ ë„êµ¬ê°€ ë‹¨ìˆœíˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ìƒíƒœì˜ íŠ¹ì • í•„ë“œë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì¸ê°„ ê²€í† ë¥¼ ìš”ì²­í•˜ê³  í”¼ë“œë°±ì— ë”°ë¼ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

# %%
@tool
def human_review(
    human_feedback, tool_call_id: Annotated[str, InjectedToolCallId]
) -> str:
    """Request human review for information."""
    # ì¸ê°„ì—ê²Œ ê²€í†  ìš”ì²­
    human_response = interrupt(
        {"question": "ì´ ì •ë³´ê°€ ë§ë‚˜ìš”?", "human_feedback": human_feedback}
    )

    feedback = human_response.get("human_feedback", "")

    if feedback.strip() == "":
        # ì‚¬ìš©ìê°€ AI ì˜ ë‹µë³€ì— ë™ì˜í•˜ëŠ” ê²½ìš°
        return Command(
            update={
                "messages": [ToolMessage(human_response, tool_call_id=tool_call_id)]
            }
        )
    else:
        # ì‚¬ìš©ìê°€ AI ì˜ ë‹µë³€ì— ë™ì˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        corrected_information = f"# ì‚¬ìš©ìì— ì˜í•´ ìˆ˜ì •ëœ í”¼ë“œë°±: {feedback}"
        return Command(
            update={
                "messages": [
                    ToolMessage(corrected_information, tool_call_id=tool_call_id)
                ]
            }
        )


# %% [markdown]
# ### ì»¤ìŠ¤í…€ ìƒíƒœ ê·¸ë˜í”„
#
# `CustomState`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ê¸°ë³¸ `State` ëŒ€ì‹  ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ë©´ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í•„ìš”í•œ ì¶”ê°€ ë°ì´í„°ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì»¤ìŠ¤í…€ ìƒíƒœì™€ `human_review` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

# %%
# ë„êµ¬ ë¦¬ìŠ¤íŠ¸
tools_custom = [human_review]

# ìƒˆë¡œìš´ ê·¸ë˜í”„ êµ¬ì„±
custom_graph_builder = StateGraph(CustomState)  # CustomState ì‚¬ìš©

# LLMì— ë„êµ¬ ë°”ì¸ë”©
llm_with_custom_tools = llm.bind_tools(tools_custom)


def chatbot_custom(state: CustomState):
    """ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡"""
    message = llm_with_custom_tools.invoke(state["messages"])

    if hasattr(message, "tool_calls"):
        assert len(message.tool_calls) <= 1

    return {"messages": [message]}


# ë…¸ë“œì™€ ì—£ì§€ ì¶”ê°€
custom_graph_builder.add_node("chatbot", chatbot_custom)
tool_node_custom = ToolNode(tools=tools_custom)
custom_graph_builder.add_node("tools", tool_node_custom)

custom_graph_builder.add_conditional_edges("chatbot", tools_condition)
custom_graph_builder.add_edge("tools", "chatbot")
custom_graph_builder.add_edge(START, "chatbot")

# ì»´íŒŒì¼
memory_custom = InMemorySaver()
custom_graph = custom_graph_builder.compile(checkpointer=memory_custom)

# %% [markdown]
# ### ê·¸ë˜í”„ ì‹œê°í™”
#
# ì»¤ìŠ¤í…€ ìƒíƒœ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. ê¸°ë³¸ ë„êµ¬ ê·¸ë˜í”„ì™€ ë™ì¼í•œ êµ¬ì¡°ì´ì§€ë§Œ, ë‚´ë¶€ì ìœ¼ë¡œ `CustomState`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì»¤ìŠ¤í…€ ìƒíƒœ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

# %%
# ê·¸ë˜í”„ ì‹œê°í™”
visualize_graph(custom_graph)

# %% [markdown]
# ### ì»¤ìŠ¤í…€ ìƒíƒœ í…ŒìŠ¤íŠ¸
#
# `human_review` ë„êµ¬ í˜¸ì¶œ ì‹œ interruptë¡œ ì¤‘ë‹¨ë˜ê³ , ì¬ê°œ ì‹œ ì‚¬ìš©ì í”¼ë“œë°±ì— ë”°ë¼ ìƒíƒœê°€ ê°±ì‹ ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë¹ˆ í”¼ë“œë°±ì„ ì œê³µí•˜ë©´ AIì˜ ë‹µë³€ì— ë™ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì¸ê°„ ê²€í† ê°€ í•„ìš”í•œ ì§ˆë¬¸ìœ¼ë¡œ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

# %%
# LangGraphì˜ ì¶œì‹œì¼ì„ ì¡°ì‚¬í•˜ê³  ê²€í†  ìš”ì²­
user_input = (
    "2024ë…„ ë…¸ë²¨ ë¬¸í•™ìƒ ìˆ˜ìƒìê°€ ëˆ„êµ¬ì¸ì§€ ì¡°ì‚¬í•´ì£¼ì„¸ìš”. "
    "ë‹µì„ ì°¾ìœ¼ë©´ `human_review` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ê²€í† ë¥¼ ìš”ì²­í•˜ì„¸ìš”."
)

custom_config = RunnableConfig(configurable={"thread_id": random_uuid()})

print(f"User: {user_input}\n")

# ì‹¤í–‰ (interruptì—ì„œ ì¤‘ë‹¨ë  ê²ƒì„)
stream_graph(
    custom_graph,
    inputs={"messages": [HumanMessage(content=user_input)]},
    config=custom_config,
)

# %%
from langchain_teddynote.messages import display_message_tree

# ìµœì‹  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
last_message = custom_graph.get_state(custom_config).values["messages"][-1]

# ìµœì‹  ë©”ì‹œì§€ tree êµ¬ì¡°ë¡œ í‘œì‹œ
display_message_tree(last_message)

# %%
# AI ê°€ ì‘ì„±í•œ ë‚´ìš©
print(last_message.tool_calls[0]["args"]["human_feedback"])

# %%
# ì¸ê°„ì˜ ê²€í†  ì‘ë‹µìœ¼ë¡œ ì¬ê°œ
human_command = Command(
    resume={"human_feedback": "2024ë…„ ë…¸ë²¨ ë¬¸í•™ìƒ ìˆ˜ìƒìëŠ” ëŒ€í•œë¯¼êµ­ì˜ í•œê°• ì‘ê°€ì…ë‹ˆë‹¤."}
)

stream_graph(custom_graph, inputs=human_command, config=custom_config)

# %% [markdown]
# ---
#
# ## ìƒíƒœ ì´ë ¥ ê´€ë¦¬
#
# ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ìƒíƒœë¥¼ ì €ì¥/ë³µì›í•˜ì—¬ ë¡¤ë°±/ì¬ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì„ í†µí•´ íŠ¹ì • ì‹œì ìœ¼ë¡œ ëŒì•„ê°€ ë‹¤ë¥¸ ê²½ë¡œë¡œ ì‹¤í–‰í•˜ê±°ë‚˜, ì‹¤í–‰ ì´ë ¥ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ### í•µì‹¬ ê°œë…
#
# | ê°œë… | ì„¤ëª… |
# |------|------|
# | State History | ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ë°œìƒí•œ ëª¨ë“  ìƒíƒœ ë³€ê²½ ì´ë ¥ |
# | Checkpoint ID | íŠ¹ì • ì‹œì ì˜ ìƒíƒœë¥¼ ì‹ë³„í•˜ëŠ” ê³ ìœ  ID |
# | Rollback | ì§€ì •í•œ ì²´í¬í¬ì¸íŠ¸ë¡œ ìƒíƒœë¥¼ ë³µì› |
# | Resume | ë³µì›ëœ ìƒíƒœì—ì„œ ì‹¤í–‰ì„ ì¬ê°œ |
#
# ì•„ë˜ ì„¹ì…˜ì—ì„œëŠ” ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤.

# %% [markdown]
# ### ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì„±
#
# ìƒíƒœ ì´ë ¥ í™•ì¸ê³¼ ë¡¤ë°±/ì¬ì‹¤í–‰ì„ ìœ„í•œ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. Tavily ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ë²ˆì˜ ëŒ€í™”ë¥¼ ìˆ˜í–‰í•˜ê³ , ê° ë‹¨ê³„ì˜ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì²´í¬í¬ì¸í„°ê°€ ì—°ê²°ëœ ê²€ìƒ‰ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

# %%
# ìƒíƒœ ê´€ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ê·¸ë˜í”„
graph_builder = StateGraph(State)

# ë„êµ¬ì™€ LLM ì„¤ì •
tools = [TavilySearch(max_results=2)]
llm_with_tools_tt = llm.bind_tools(tools)


def chatbot_tt(state: State):
    """ìƒíƒœ ê´€ë¦¬ í…ŒìŠ¤íŠ¸ìš© ì±—ë´‡"""
    return {"messages": [llm_with_tools_tt.invoke(state["messages"])]}


# ê·¸ë˜í”„ êµ¬ì„±
graph_builder.add_node("chatbot", chatbot_tt)
tool_node_tt = ToolNode(tools=tools)
graph_builder.add_node("tools", tool_node_tt)

graph_builder.add_conditional_edges("chatbot", tools_condition)
graph_builder.add_edge("tools", "chatbot")
graph_builder.add_edge(START, "chatbot")

# ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼
memory_tt = InMemorySaver()
time_travel_graph = graph_builder.compile(checkpointer=memory_tt)

# %% [markdown]
# ### ê·¸ë˜í”„ ì‹œê°í™”
#
# ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ê·¸ë˜í”„ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì´ì „ì— êµ¬ì„±í•œ ë„êµ¬ ê·¸ë˜í”„ì™€ ë™ì¼í•œ êµ¬ì¡°ì…ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.

# %%
# ì‹œê°í™”
visualize_graph(time_travel_graph)

# %% [markdown]
# ### ì²´í¬í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ ìƒì„±
#
# ì—¬ëŸ¬ ë²ˆ ëŒ€í™”ë¥¼ ì‹¤í–‰í•˜ì—¬ ìƒíƒœ ì´ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ê° ì‹¤í–‰ë§ˆë‹¤ ìƒˆë¡œìš´ ì²´í¬í¬ì¸íŠ¸ê°€ ìƒì„±ë˜ì–´ ë‚˜ì¤‘ì— íŠ¹ì • ì‹œì ìœ¼ë¡œ ë¡¤ë°±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì²« ë²ˆì§¸ ê²€ìƒ‰ ëŒ€í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

# %%
time_travel_config = RunnableConfig(configurable={"thread_id": "time-travel-1"})

# ì²« ë²ˆì§¸ ëŒ€í™”
stream_graph(
    time_travel_graph,
    inputs={"messages": [HumanMessage(content="í…Œë””ë…¸íŠ¸ì— ëŒ€í•´ì„œ ì¡°ì‚¬ ì¢€ í•´ì£¼ì„¸ìš”.")]},
    config=time_travel_config,
)

# %%
# ë‘ ë²ˆì§¸ ëŒ€í™”
stream_graph(
    time_travel_graph,
    inputs={
        "messages": [HumanMessage(content="í…Œë””ë…¸íŠ¸ ì˜¨ë¼ì¸ ê°•ì˜ ì£¼ì†Œë¥¼ ì¡°ì‚¬ í•´ì£¼ì„¸ìš”.")]
    },
    config=time_travel_config,
)

# %% [markdown]
# ### ìƒíƒœ ì´ë ¥ íƒìƒ‰
#
# `get_state_history`ë¡œ ì „ì²´ ìƒíƒœ ì´ë ¥ì„ ì¡°íšŒí•˜ê³  ë¡¤ë°±í•  ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì´ë ¥ì€ ìµœì‹ ìˆœìœ¼ë¡œ ë°˜í™˜ë˜ë©°, ê° ì²´í¬í¬ì¸íŠ¸ì—ëŠ” í•´ë‹¹ ì‹œì ì˜ ìƒíƒœ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ìƒíƒœ ì´ë ¥ì„ ì¡°íšŒí•˜ê³  íŠ¹ì • ì¡°ê±´(ë©”ì‹œì§€ ìˆ˜ê°€ 6ê°œ)ì— í•´ë‹¹í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

# %%
# ì „ì²´ ìƒíƒœ íˆìŠ¤í† ë¦¬ í™•ì¸
print("ìƒíƒœ íˆìŠ¤í† ë¦¬ (ìµœì‹ ìˆœ):")
print("=" * 80)

# to_replay ë³€ìˆ˜ ì´ˆê¸°í™”
to_replay = None

for i, state in enumerate(time_travel_graph.get_state_history(time_travel_config)):
    print(f"\n[ì²´í¬í¬ì¸íŠ¸ {i}]")
    print(f"  ë‹¤ìŒ ë…¸ë“œ: {state.next}")
    print(f"  ì²´í¬í¬ì¸íŠ¸ ID: {state.config['configurable']['checkpoint_id']}")

    if len(state.values["messages"]) == 6 and to_replay is None:
        print("  ì´ ìƒíƒœë¡œ ë˜ëŒì•„ê°ˆ ì˜ˆì •")
        display_message_tree(state.values["messages"][-1])
        to_replay = state


print("\n" + "=" * 80)

# %% [markdown]
# ### ì²´í¬í¬ì¸íŠ¸ë¡œ ë¡¤ë°±
#
# ì„ íƒí•œ ì²´í¬í¬ì¸íŠ¸ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì´ ì‹œì ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ë‚´ìš©ì„ í‘œì‹œí•˜ì—¬ ì–´ëŠ ì§€ì ìœ¼ë¡œ ëŒì•„ê°ˆì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ì„ íƒí•œ ì²´í¬í¬ì¸íŠ¸ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

# %%
display_message_tree(to_replay.values["messages"][-1])

# %% [markdown]
# ### ìƒíƒœ ìˆ˜ì •
#
# ë³µì›ëœ ìƒíƒœì—ì„œ ë„êµ¬ í˜¸ì¶œ íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤. `update_tool_call` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ì¡´ ë„êµ¬ í˜¸ì¶œì˜ ì¸ìë¥¼ ë³€ê²½í•  ìˆ˜ ìˆì–´, ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì¬ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” Tavily ê²€ìƒ‰ ë„êµ¬ì˜ ì¿¼ë¦¬ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.

# %%
from langchain_teddynote.tools import update_tool_call

# ì‚¬ìš© ì˜ˆì‹œ:
updated_message = update_tool_call(
    to_replay.values["messages"][-1],
    tool_name="tavily_search",
    tool_args={"query": "í…Œë””ë…¸íŠ¸ ì˜¨ë¼ì¸ ê°•ì˜ site:naver.com", "search_depth": "basic"},
)

# %%
updated_message

# %%
# ë³€ê²½í•˜ê¸° ì „ì˜ message
display_message_tree(to_replay.values["messages"][-1])

# %%
# ë³€ê²½í•œ ì´í›„ì˜ ë©”ì‹œì§€ íŠ¸ë¦¬
display_message_tree(updated_message)

# %%
# ë³€ê²½ëœ ë©”ì‹œì§€ë¥¼ update_state ë¡œ ì—…ë°ì´íŠ¸
updated_state = time_travel_graph.update_state(
    values={"messages": [updated_message]}, config=to_replay.config
)

# %% [markdown]
# ### ìˆ˜ì • ìƒíƒœ ì¬ì‹¤í–‰
#
# ì—…ë°ì´íŠ¸ëœ ìƒíƒœë¡œ ê·¸ë˜í”„ë¥¼ ì¬ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. `update_state` ë©”ì„œë“œë¡œ ìƒíƒœë¥¼ ìˆ˜ì •í•˜ë©´ í•´ë‹¹ ì‹œì ë¶€í„° ìƒˆë¡œìš´ ê²½ë¡œë¡œ ì‹¤í–‰ì´ ê³„ì†ë©ë‹ˆë‹¤.
#
# ì•„ë˜ ì½”ë“œëŠ” ìˆ˜ì •ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ê·¸ë˜í”„ë¥¼ ì¬ì‹¤í–‰í•©ë‹ˆë‹¤.

# %%
# ì—…ë°ì´íŠ¸ëœ ë©”ì‹œì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í•©ë‹ˆë‹¤.
stream_graph(time_travel_graph, inputs=None, config=updated_state)
