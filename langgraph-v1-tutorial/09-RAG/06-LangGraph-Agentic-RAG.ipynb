{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517af4d9",
   "metadata": {},
   "source": [
    "# Agentic RAG\n",
    "\n",
    "이번 튜토리얼에서는 **에이전트(Agent)** 기반의 RAG 시스템을 구축합니다.\n",
    "\n",
    "에이전트는 검색 도구를 사용할지 여부를 스스로 결정합니다. 에이전트에 대한 자세한 내용은 [Agent 튜토리얼](https://wikidocs.net/233782)을 참고하세요.\n",
    "\n",
    "**학습 목표**\n",
    "\n",
    "- LLM에 검색 도구를 바인딩하여 에이전트를 구성하는 방법을 학습합니다.\n",
    "- `ToolNode`와 `tools_condition`을 활용한 에이전트 그래프를 구축합니다.\n",
    "- 문서 관련성 평가 및 쿼리 재작성 흐름을 구현합니다.\n",
    "\n",
    "![langgraph-agentic-rag](assets/langgraph-agentic-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dc8f5",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4def9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1966105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다.\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangGraph-RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee354d2",
   "metadata": {},
   "source": [
    "## PDF 기반 Retrieval Chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d62a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 로드합니다.\n",
    "pdf = PDFRetrievalChain([\"data/SPRI_AI_Brief_2023년12월호_F.pdf\"]).create_chain()\n",
    "\n",
    "# retriever와 chain을 생성합니다.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575f296",
   "metadata": {},
   "source": [
    "## 검색 도구 생성\n",
    "\n",
    "`create_retriever_tool`을 사용하여 Retriever를 도구로 변환합니다.\n",
    "\n",
    "**`document_prompt` 사용 가능한 키**\n",
    "\n",
    "- `page_content`: 문서 내용\n",
    "- `source`: 문서 출처\n",
    "- `page`: 페이지 번호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d6f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# PDF 문서 검색 도구 생성\n",
    "retriever_tool = create_retriever_tool(\n",
    "    pdf_retriever,\n",
    "    \"pdf_retriever\",\n",
    "    \"Search and return information about SPRI AI Brief PDF file. It contains useful information on recent AI trends. The document is published on Dec 2023.\",\n",
    "    document_prompt=PromptTemplate.from_template(\n",
    "        \"<document><context>{page_content}</context><metadata><source>{source}</source><page>{page}</page></metadata></document>\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 도구 리스트에 추가\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b169f21",
   "metadata": {},
   "source": [
    "## Agent State 정의\n",
    "\n",
    "에이전트의 상태는 `messages` 목록으로 구성됩니다. 각 노드는 이 목록에 메시지를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55abb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# 에이전트 상태 정의 (TypedDict 기반 - LangGraph v1 호환)\n",
    "class AgentState(TypedDict):\n",
    "    # add_messages 리듀서를 사용하여 메시지 시퀀스 관리\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f93141",
   "metadata": {},
   "source": [
    "## 노드와 엣지\n",
    "\n",
    "에이전트 기반 RAG 그래프는 다음과 같이 구성됩니다.\n",
    "\n",
    "- **상태**: 메시지들의 집합\n",
    "- **노드**: 상태를 업데이트(추가)\n",
    "- **조건부 엣지**: 다음에 방문할 노드를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17b98e",
   "metadata": {},
   "source": [
    "### 문서 평가기(Grader) 및 노드 정의\n",
    "\n",
    "검색된 문서의 관련성을 평가하는 Grader와 각 노드를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 최신 모델 이름 가져오기\n",
    "MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "\n",
    "\n",
    "# 관련성 평가를 위한 데이터 모델\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"검색된 문서의 관련성을 평가하는 이진 점수입니다.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"문서가 질문과 관련이 있으면 'yes', 없으면 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"검색된 문서의 관련성을 평가하는 함수입니다.\"\"\"\n",
    "    # LLM 초기화\n",
    "    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)\n",
    "\n",
    "    # 구조화된 출력을 위한 LLM 설정\n",
    "    llm_with_tool = model.with_structured_output(GradeDocuments)\n",
    "\n",
    "    # 프롬프트 템플릿 정의\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # 체인 생성\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    # 현재 상태에서 메시지 추출\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content\n",
    "    retrieved_docs = last_message.content\n",
    "\n",
    "    # 관련성 평가 실행\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": retrieved_docs})\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    # 관련성 여부에 따른 결정\n",
    "    if score == \"yes\":\n",
    "        print(\"==== [DECISION: DOCS RELEVANT] ====\")\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        print(\"==== [DECISION: DOCS NOT RELEVANT] ====\")\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"에이전트 노드입니다. 도구 사용 여부를 결정합니다.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # LLM 초기화 및 도구 바인딩\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=MODEL_NAME)\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # 에이전트 응답 생성\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"질문을 재작성하는 노드입니다.\"\"\"\n",
    "    print(\"==== [QUERY REWRITE] ====\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    # 질문 개선을 위한 프롬프트 구성\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"\\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # LLM으로 질문 개선\n",
    "    model = ChatOpenAI(temperature=0, model=MODEL_NAME, streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"답변을 생성하는 노드입니다.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    docs = messages[-1].content\n",
    "\n",
    "    # RAG 프롬프트 가져오기\n",
    "    prompt = hub.pull(\"teddynote/rag-prompt\")\n",
    "\n",
    "    # LLM 초기화 및 체인 구성\n",
    "    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0, streaming=True)\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # 답변 생성\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481eb52",
   "metadata": {},
   "source": [
    "## 그래프 생성\n",
    "\n",
    "에이전트 그래프를 구성합니다.\n",
    "\n",
    "- `agent`로 시작하여 함수 호출 여부를 결정합니다.\n",
    "- 함수 호출 시 `retrieve` 노드에서 도구를 실행합니다.\n",
    "- 도구 출력을 상태에 추가하여 에이전트를 다시 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# AgentState 기반 워크플로우 초기화\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"agent\", agent)  # 에이전트 노드\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # 검색 노드\n",
    "workflow.add_node(\"rewrite\", rewrite)  # 질문 재작성 노드\n",
    "workflow.add_node(\"generate\", generate)  # 답변 생성 노드\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# 검색 여부 결정을 위한 조건부 엣지\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,  # 에이전트 결정 평가\n",
    "    {\n",
    "        \"tools\": \"retrieve\",  # 도구 호출 -> 검색\n",
    "        END: END,  # 도구 미호출 -> 종료\n",
    "    },\n",
    ")\n",
    "\n",
    "# 검색 후 문서 평가를 위한 조건부 엣지\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,  # 문서 품질 평가\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd907104",
   "metadata": {},
   "source": [
    "### 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df716e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659e932",
   "metadata": {},
   "source": [
    "## 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b1efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import stream_graph, invoke_graph, random_uuid\n",
    "\n",
    "# config 설정\n",
    "config = RunnableConfig(recursion_limit=10, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "# 문서 검색이 필요한 질문\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"삼성전자가 개발한 생성형 AI 의 이름은?\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(graph, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da64d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 스트리밍 출력\n",
    "stream_graph(graph, inputs, config, [\"agent\", \"rewrite\", \"generate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577bd32",
   "metadata": {},
   "source": [
    "### 문서 검색이 불필요한 질문 예시\n",
    "\n",
    "에이전트가 검색 도구를 사용하지 않고 바로 답변하는 경우입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1bc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 검색이 불필요한 질문 예시\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"대한민국의 수도는?\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(graph, inputs, config, [\"agent\", \"rewrite\", \"generate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac602bd",
   "metadata": {},
   "source": [
    "### 재귀 상태 처리\n",
    "\n",
    "문서 검색이 불가능한 질문의 경우, 지속적인 검색 시도로 인해 `GraphRecursionError`가 발생할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import GraphRecursionError\n",
    "\n",
    "# 문서 검색이 불가능한 질문\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"테디노트의 랭체인 튜토리얼에 대해서 알려줘\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 그래프 실행\n",
    "    stream_graph(graph, inputs, config, [\"agent\", \"rewrite\", \"generate\"])\n",
    "except GraphRecursionError as recursion_error:\n",
    "    print(f\"GraphRecursionError: {recursion_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이 튜토리얼에서는 **Agentic RAG** 시스템을 구축했습니다.\n",
    "\n",
    "### 핵심 개념\n",
    "\n",
    "1. **Agent**: 도구 사용 여부를 스스로 결정하는 LLM 기반 에이전트입니다.\n",
    "2. **ToolNode**: 도구를 노드로 래핑하여 그래프에서 사용합니다.\n",
    "3. **tools_condition**: 에이전트의 도구 호출 결정을 기반으로 라우팅합니다.\n",
    "4. **문서 평가**: 검색된 문서의 관련성을 평가하여 답변 생성 또는 재검색을 결정합니다.\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "다음 튜토리얼에서는 **CRAG(Corrective RAG)**를 구현하여 검색 결과를 정제하고 웹 검색으로 보강하는 방법을 학습합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
